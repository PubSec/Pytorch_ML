{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1+cu124'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create a straight line dataset using the linear regression formula (weight * X + bias).\n",
    "\n",
    "1. Set weight=0.3 and bias=0.9 there should be at least 100 datapoints total.\n",
    "2. Split the data into 80% training, 20% testing.\n",
    "3. Plot the training and testing data so it becomes visual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of X samples: 100\n",
      "Number of y samples: 100\n",
      "First 10 X & y samples:\n",
      "X: tensor([[0.0000],\n",
      "        [0.0100],\n",
      "        [0.0200],\n",
      "        [0.0300],\n",
      "        [0.0400],\n",
      "        [0.0500],\n",
      "        [0.0600],\n",
      "        [0.0700],\n",
      "        [0.0800],\n",
      "        [0.0900]])\n",
      "y: tensor([[0.9000],\n",
      "        [0.9030],\n",
      "        [0.9060],\n",
      "        [0.9090],\n",
      "        [0.9120],\n",
      "        [0.9150],\n",
      "        [0.9180],\n",
      "        [0.9210],\n",
      "        [0.9240],\n",
      "        [0.9270]])\n"
     ]
    }
   ],
   "source": [
    "weights = 0.3\n",
    "bias = 0.9\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.01\n",
    "X = torch.arange(start,end,step,dtype=torch.float).unsqueeze(dim=1)\n",
    "y = weights * X + bias\n",
    "print(f\"Number of X samples: {len(X)}\")\n",
    "print(f\"Number of y samples: {len(y)}\")\n",
    "print(f\"First 10 X & y samples:\\nX: {X[:10]}\\ny: {y[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 20, 80, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split = int(0.8*len(X))\n",
    "X_train,y_train = X[:train_test_split],y[:train_test_split]\n",
    "X_test,y_test = X[train_test_split:],y[train_test_split:]\n",
    "len(X_train),len(X_test),len(y_train),len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,test_data=X_test,train_label=y_train,test_label=y_test,predictions=None):\n",
    "    \"\"\"\n",
    "    Plot the train,test data and prediction if available\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "    \n",
    "    plt.scatter(train_data,train_label,c=\"b\",label=\"Training data\")\n",
    "\n",
    "    plt.scatter(test_data,test_label,c=\"g\",label=\"Testing data\")\n",
    "\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data,predictions,c=\"r\",label = \"Predictions\")\n",
    "\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAJGCAYAAACZel7oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJnklEQVR4nO3de3RU5aH+8WcSyIXCBLnlNgNBjIKKIKhprFHowQahHC51QaHV1CrWFi+YWoXKIaBt8VQUqGJVrMa7IAXaVW0spnAQxRuSc/QnUJFEMIZAQDNcNMBk//6YZmAmM5OZSSZz2d/PWllmdvY7eafuRXnc7/tsi2EYhgAAAADAZJKiPQEAAAAAiAbCEAAAAABTIgwBAAAAMCXCEAAAAABTIgwBAAAAMCXCEAAAAABTIgwBAAAAMKUu0Z5AR2lubtYXX3yhHj16yGKxRHs6AAAAAKLEMAwdPnxYOTk5Skryf/8nYcLQF198IbvdHu1pAAAAAIgRe/fulc1m8/vzhAlDPXr0kOT6wFarNcqzAQAAABAtDodDdrvdnRH8SZgw1LI0zmq1EoYAAAAAtLl9hgIFAAAAAKZEGAIAAABgSoQhAAAAAKaUMHuGgtHc3Kzjx49HexqIcV27dlVycnK0pwEAAIAIM00YOn78uKqrq9Xc3BztqSAO9OzZU1lZWTyzCgAAIIGZIgwZhqG6ujolJyfLbrcHfPASzM0wDB07dkz79++XJGVnZ0d5RgAAAIgUU4ShkydP6tixY8rJyVG3bt2iPR3EuPT0dEnS/v371a9fP5bMAQAAJChT3CJxOp2SpJSUlCjPBPGiJTSfOHEiyjMBAABApJgiDLVg/weCxbUCAACQ+EwVhgAAAACgBWHIZPLy8rR06dKgz9+4caMsFou++uqriM3Jn/LycvXs2bPTfy8AAADMgTAUoywWS8CvBQsWhPW+7733nm688cagz7/00ktVV1enjIyMsH5fZws17AEAAMC8TNEm11GcTumNN6S6Oik7WyoqkiJVNFZXV+f+fuXKlZo/f7527tzpPta9e3f394ZhyOl0qkuXtv919u3bN6R5pKSkKCsrK6QxAAAAQDwI+c7Qpk2bNGHCBOXk5MhisWjdunUBz1+zZo2uvPJK9e3bV1arVYWFhXrttddanbd8+XLl5eUpLS1NBQUFevfdd0OdWkStWSPl5UmjR0szZrj+mZfnOh4JWVlZ7q+MjAxZLBb36x07dqhHjx76+9//rpEjRyo1NVWbN2/Wp59+qokTJyozM1Pdu3fXxRdfrNdff93jfb3vnFgsFj3xxBOaPHmyunXrpvz8fP31r391/9x7mVzL0rXXXntNQ4YMUffu3TV27FiP8Hby5Endeuut6tmzp3r37q277rpLJSUlmjRpUsDPXF5erv79+6tbt26aPHmyDh486PHztj7fqFGj9Nlnn+n2229330GTpIMHD2r69OnKzc1Vt27dNHToUL344ouh/OsAAABAAgo5DB09elTDhg3T8uXLgzp/06ZNuvLKK/Xqq69q69atGj16tCZMmKBt27a5z1m5cqVKS0tVVlamDz74QMOGDVNxcbH7wZfRtmaNdPXV0uefex6vrXUdj1QgasucOXN03333afv27brgggt05MgRjRs3TpWVldq2bZvGjh2rCRMmaM+ePQHfZ+HChZo6dar+7//+T+PGjdOPfvQjHTp0yO/5x44d0+LFi/Xss89q06ZN2rNnj+644w73z//7v/9bzz//vJ566im9+eabcjgcbYbmd955R9dff71uvvlmVVVVafTo0frNb37jcU5bn2/NmjWy2Wy65557VFdX5w5o33zzjUaOHKlXXnlFH330kW688UZdc801MRe4AQAA0MmMdpBkrF27NuRx5557rrFw4UL360suucSYNWuW+7XT6TRycnKMRYsWBf2ejY2NhiSjsbGx1c++/vpr4+OPPza+/vrrkOd68qRh2GyGIfn+slgMw253nRcpTz31lJGRkeF+vWHDBkOSsW7dujbHnnfeecZDDz3kfj1gwABjyZIl7teSjHnz5rlfHzlyxJBk/P3vf/f4XV9++aV7LpKMXbt2uccsX77cyMzMdL/OzMw07r//fvfrkydPGv379zcmTpzod57Tp083xo0b53Fs2rRpHp87nM/nz/jx441f/vKXfn/enmsGAAAA0RUoG5yu0wsUmpubdfjwYfXq1UuSdPz4cW3dulVjxoxxn5OUlKQxY8Zoy5Ytft+nqalJDofD4ysS3nij9R2h0xmGtHev67zOdtFFF3m8PnLkiO644w4NGTJEPXv2VPfu3bV9+/Y27wxdcMEF7u+/9a1vyWq1Brwr161bNw0aNMj9Ojs7231+Y2Oj6uvrdckll7h/npycrJEjRwacw/bt21VQUOBxrLCwsEM+n9Pp1L333quhQ4eqV69e6t69u1577bU2xwEAACCxdXqBwuLFi3XkyBFNnTpVktTQ0CCn06nMzEyP8zIzM7Vjxw6/77No0SItXLgwonOVXGUJHXleR/rWt77l8fqOO+7Q+vXrtXjxYp111llKT0/X1VdfrePHjwd8n65du3q8tlgsam5uDul8wzBCnH3owv18999/v5YtW6alS5dq6NCh+ta3vqXZs2e3OQ4AAABtczY79caeN1R3uE7ZPbJV1L9IyUkRahnrYJ0ahl544QUtXLhQf/nLX9SvX792vdfcuXNVWlrqfu1wOGS329s7xVayszv2vEh688039ZOf/ESTJ0+W5LqTUlNT06lzyMjIUGZmpt577z1dfvnlklx3Zj744AMNHz7c77ghQ4bonXfe8Tj29ttve7wO5vOlpKTI6XS2Gjdx4kT9+Mc/luS6O/mvf/1L5557bjgfEQAAAP+2Zvsa3VZxmz53nFpKZbPatGzsMk0ZMiWKMwtOpy2Te+mll3TDDTdo1apVHkvi+vTpo+TkZNXX13ucX19fH7DSOTU1VVar1eMrEoqKJJtN+ncxWSsWi2S3u86Ltvz8fK1Zs0ZVVVX63//9X82YMSPgHZ5IueWWW7Ro0SL95S9/0c6dO3Xbbbfpyy+/dLe7+XLrrbeqoqJCixcv1ieffKKHH35YFRUVHucE8/ny8vK0adMm1dbWqqGhwT1u/fr1euutt7R9+3b97Gc/a3W9AQAAIDRrtq/R1auu9ghCklTrqNXVq67Wmu1RahkLQaeEoRdffFHXXXedXnzxRY0fP97jZykpKRo5cqQqKyvdx5qbm1VZWdlqz0g0JCdLy5a5vvf+u3zL66VLI/e8oVA8+OCDOuOMM3TppZdqwoQJKi4u1ogRIzp9HnfddZemT5+ua6+9VoWFherevbuKi4uVlpbmd8y3v/1trVixQsuWLdOwYcP0j3/8Q/PmzfM4J5jPd88996impkaDBg1yP1Np3rx5GjFihIqLizVq1ChlZWW1WfMNAAAA/5zNTt1WcZsMtd4q0XJsdsVsOZudrX4eSyxGiJs9jhw5ol27dkmSLrzwQj344IMaPXq0evXqpf79+2vu3Lmqra3VM888I8m1NK6kpETLli3TlCmnbpWlp6crIyNDkqtau6SkRI899pguueQSLV26VKtWrdKOHTta7SXyx+FwKCMjQ42Nja3uEn3zzTeqrq7WwIEDA/6FPJA1a6TbbvMsU7DbXUFoSuzfAYyq5uZmDRkyRFOnTtW9994b7ekEpSOuGQAAgES1sWajRj89us3zNpRs0Ki8UZGfkJdA2eB0Ie8Zev/99zV69KkP3rJvp6SkROXl5aqrq/No6Xr88cd18uRJzZo1S7NmzXIfbzlfkqZNm6YDBw5o/vz52rdvn4YPH66Kioqgg1BnmDJFmjjR1RpXV+faI1RUFBt3hGLNZ599pn/84x+64oor1NTUpIcffljV1dWaMWNGtKcGAACAMHiXJNQ6aoMaV3c4Ci1jIQj5zlCsivSdIQRv7969+uEPf6iPPvpIhmHo/PPP13333ecuVIgHXDMAAAAuvkoS+nTro4ZjDW2OTbg7Q0Bb7Ha73nzzzWhPAwAAAO3UUpLgvTeorSBkkUU2q01F/WOgZSyATn/oKgAAAIDYF6gk4XQWWXy+Xjp2acw/b4gwBAAAAKCVN/a80ao225c+3fp4vLZZbVo9dXVcPGeIZXIAAAAAwi5JWFK8RLnWXPe4ov5FMX9HqAVhCAAAADA5fyUJwci15kalJKEjEIYAAAAAE0v0koRA2DMEAAAAmJQZShICIQxBkrRgwQINHz48Kr/7Jz/5iSZNmhSV3w0AAGBmZihJCIRlcjHKYrEE/HlZWZkWLFgQ9nuvXbvWI4DccccduuWWW8J6v85WU1OjgQMHatu2bVELcAAAAPHIjCUJgRCGQuB98UTyIqirq3N/v3LlSs2fP187d+50H+vevXuH/r7u3bt3+HsCAAAgdpi1JCEQlskFac32NcpblqfRT4/WjDUzNPrp0cpblqc129dE5PdlZWW5vzIyMmSxWDyOvfTSSxoyZIjS0tI0ePBgPfLII+6xx48f180336zs7GylpaVpwIABWrRokSQpLy9PkjR58mRZLBb3a+9lci1L1xYvXqzs7Gz17t1bs2bN0okTJ9zn1NXVafz48UpPT9fAgQP1wgsvKC8vT0uXLvX7uZxOp0pLS9WzZ0/17t1bd955pwzDc41qRUWFLrvsMvc53//+9/Xpp5+6fz5w4EBJ0oUXXiiLxaJRo0ZJkt577z1deeWV6tOnjzIyMnTFFVfogw8+CPV/egAAgITTUpLgvSQumJIEu9Ue1yUJgRCGguDv4ql11OrqVVdHLBD58/zzz2v+/Pn67W9/q+3bt+t3v/ud/uu//ktPP/20JOkPf/iD/vrXv2rVqlXauXOnnn/+eXfoee+99yRJTz31lOrq6tyvfdmwYYM+/fRTbdiwQU8//bTKy8tVXl7u/vm1116rL774Qhs3btSf//xnPf7449q/f3/AuT/wwAMqLy/Xk08+qc2bN+vQoUNau3atxzlHjx5VaWmp3n//fVVWViopKUmTJ09Wc3OzJOndd9+VJL3++uuqq6vTmjWu//0PHz6skpISbd68WW+//bby8/M1btw4HT58OPj/cQEAABKM2UsSAmGZXBsCXTyGDFlk0eyK2Zp4zsROu0jKysr0wAMPaMoU14a1gQMH6uOPP9Zjjz2mkpIS7dmzR/n5+brssstksVg0YMAA99i+fftKknr27KmsrKyAv+eMM87Qww8/rOTkZA0ePFjjx49XZWWlZs6cqR07duj111/Xe++9p4suukiS9MQTTyg/Pz/gey5dulRz5851z/3RRx/Va6+95nHOD37wA4/XTz75pPr27auPP/5Y559/vvsz9O7d2+MzfPe73/UY9/jjj6tnz576n//5H33/+98POC8AAIBEFUpJwoFjB9yvbVablo5dGvclCYEQhtrQ1sVjyNBex169seeNTllHefToUX366ae6/vrrNXPmTPfxkydPKiMjQ5JriduVV16pc845R2PHjtX3v/99fe973wv5d5133nlKTj4V8LKzs/Xhhx9Kknbu3KkuXbpoxIgR7p+fddZZOuOMM/y+X2Njo+rq6lRQUOA+1qVLF1100UUeS+U++eQTzZ8/X++8844aGhrcd4T27Nmj888/3+/719fXa968edq4caP2798vp9OpY8eOac+ePSF/dgAAgHhFSULwCENtqDtc1/ZJIZzXXkeOHJEkrVixwiNUSHIHlxEjRqi6ulp///vf9frrr2vq1KkaM2aMVq9eHdLv6tq1q8dri8XiDiaRNGHCBA0YMEArVqxQTk6Ompubdf755+v48eMBx5WUlOjgwYNatmyZBgwYoNTUVBUWFrY5DgAAIFFQkhAawlAbsntkd+h57ZWZmamcnBzt3r1bP/rRj/yeZ7VaNW3aNE2bNk1XX321xo4dq0OHDqlXr17q2rWrnE5nu+Zxzjnn6OTJk9q2bZtGjhwpSdq1a5e+/PJLv2MyMjKUnZ2td955R5dffrkk1x2trVu3uu8wHTx4UDt37tSKFStUVOTaqLd582aP90lJSZGkVp/hzTff1COPPKJx48ZJkvbu3auGhsCbAgEAABJFyz537+0dwZQk2Ky2hC1JCIQw1Iai/kWyWW2qddT63DcUjYtn4cKFuvXWW5WRkaGxY8eqqalJ77//vr788kuVlpbqwQcfVHZ2ti688EIlJSXp5ZdfVlZWlnr27CnJ1ShXWVmp73znO0pNTQ24tM2fwYMHa8yYMbrxxhv1xz/+UV27dtUvf/lLpaenB3xG0m233ab77rtP+fn5Gjx4sB588EF99dVX7p+fccYZ6t27tx5//HFlZ2drz549mjNnjsd79OvXT+np6aqoqJDNZlNaWpoyMjKUn5+vZ599VhdddJEcDod+9atfKT09PeTPBgAAEG9CKUk4/RwzlCQEQptcG5KTkrVs7DJJsdOwccMNN+iJJ57QU089paFDh+qKK65QeXm5u3K6R48e+v3vf6+LLrpIF198sWpqavTqq68qKcn1r/uBBx7Q+vXrZbfbdeGFF4Y9j2eeeUaZmZm6/PLLNXnyZM2cOVM9evRQWlqa3zG//OUvdc0116ikpESFhYXq0aOHJk+e7P55UlKSXnrpJW3dulXnn3++br/9dt1///0e79GlSxf94Q9/0GOPPaacnBxNnDhRkvSnP/1JX375pUaMGKFrrrlGt956q/r16xf25wMAAIgXoZQknM5mtWn11NUJXZIQiMXwfshLnHI4HMrIyFBjY6OsVqvHz7755htVV1dr4MCBAf+iHoiv9Zd2qz3hGzZC8fnnn8tut+v111/Xf/zHf0R7Ou3SEdcMAABApPgqSfjx2h+3Oe65yc+ZoiQhUDY4HcvkgjRlyBRNPGeix0WXqBdPsP75z3/qyJEjGjp0qOrq6nTnnXcqLy/PvR8IAAAAHY+ShI5DGApBclIyF89pTpw4oV//+tfavXu3evTooUsvvVTPP/98qxY6AAAAdAxKEjoWYQhhKy4uVnFxcbSnAQAAYAqUJHQ8whAAAAAQg7z3BTmbnUGXJBw4dsD92ma1sc/dD1OFoQTpikAn4FoBAADR5GtfUK/0XkGNXVK8xBQlCR3BFGEoOdn1L//48eM8dwZBOXbsmCSx/wkAAHQ6f/uCDn19KKjxlCQEzxRhqEuXLurWrZsOHDigrl27up+3A3gzDEPHjh3T/v371bNnT3eQBgAA6AzB7gvyhZKE0JkiDFksFmVnZ6u6ulqfffZZtKeDONCzZ09lZWVFexoAAMBkgn14qjdKEsJjijAkSSkpKcrPz9fx48ejPRXEuK5du3JHCAAAdApfD08NRq/0Xh7L5ihJCI9pwpAkJSUlKS0tLdrTAAAAANr18NRVV69SclIyJQntZKowBAAAAMSC9j48dVTeKMJPB6BJAAAAAOhEoTw81ddr9gV1HMIQAAAA0ImCLUnwXjJns9q0eupq9gV1IJbJAQAAABHiXZBQ1L9IdYfrghrLw1MjjzAEAAAARICvggSb1aaZI2YGNZ6Hp0YeYQgAAADoYP4KEmodtSrbWKbe6b116OtDPvcN8fDUzsOeIQAAAKADBSpIMGR4FCNQkhBdhCEAAACgA7VVkGDI0MGvD2rBqAXKteZ6/IyShM7FMjkAAACgHbxLEmodtUGNy++Vr5rbaloVLHBHqPMQhgAAAIAw+SpJ8K7E9ie7R7aSk5IpSYgiwhAAAAAQBn8lCQ3HGgKOoyAhdrBnCAAAAAhRoJKE01GQENsIQwAAAECI2ipJaOG9ZI6ChNjCMjkAAACgDeGWJCwpXqJcay4FCTGKMAQAAAAE0J6ShFxrLgUJMYwwBAAAAPhBSUJiY88QAAAA4AMlCYmPMAQAAAD4QElC4mOZHAAAACBKEsyIMAQAAADToyTBnAhDAAAAMDVKEsyLPUMAAAAwLUoSzI0wBAAAANOiJMHcWCYHAAAA06AkAacjDAEAAMAUKEmAN8IQAAAAEh4lCfCFPUMAAABIaJQkwB/CEAAAABIaJQnwh2VyAAAASCiUJCBYhCEAAAAkDEoSEArCEAAAABICJQkIFXuGAAAAEPcoSUA4CEMAAACIe5QkIBwskwMAAEDcoSQBHYEwBAAAgLhCSQI6CmEIAAAAcYOSBHQk9gwBAAAgLlCSgI5GGAIAAEBcoCQBHY1lcgAAAIhJlCQg0ghDAAAAiDmUJKAzEIYAAAAQUyhJQGdhzxAAAABiBiUJ6EyEIQAAAMQMShLQmUIOQ5s2bdKECROUk5Mji8WidevWBTy/rq5OM2bM0Nlnn62kpCTNnj271Tnl5eWyWCweX2lpaaFODQAAAHHG2ezUxpqNevHDF7WxZmNIJQkbSjbohSkvaEPJBlXfVk0QQshC3jN09OhRDRs2TD/96U81ZUrbF1xTU5P69u2refPmacmSJX7Ps1qt2rlzp/u1xWLxey4AAADiHyUJiLaQw9BVV12lq666Kujz8/LytGzZMknSk08+6fc8i8WirKysoN+3qalJTU1N7tcOhyPosQAAAIguShIQC2Jmz9CRI0c0YMAA2e12TZw4Uf/v//2/gOcvWrRIGRkZ7i+73d5JMwUAAEB7UJKAWBETYeicc87Rk08+qb/85S967rnn1NzcrEsvvVSff+5/89zcuXPV2Njo/tq7d28nzhgAAADhoiQBsSImnjNUWFiowsJC9+tLL71UQ4YM0WOPPaZ7773X55jU1FSlpqZ21hQBAAAQBmezU2/seUN1h+uU3SNbRf2LVHe4LqixS4qXKNea6zGWO0LoSDERhrx17dpVF154oXbt2hXtqQAAACBMvgoSbFabZo6YGdR4ShIQaTGxTM6b0+nUhx9+qOzs7GhPBQAAAGFoKUjwXg5X66hV2cYy9U7v3WpPUAuLLLJb7ZQkIOJCvjN05MgRjzs21dXVqqqqUq9evdS/f3/NnTtXtbW1euaZZ9znVFVVucceOHBAVVVVSklJ0bnnnitJuueee/Ttb39bZ511lr766ivdf//9+uyzz3TDDTe08+MBAACgswUqSDBkeIQgiywe51GSgM4Uchh6//33NXr0aPfr0tJSSVJJSYnKy8tVV1enPXv2eIy58MIL3d9v3bpVL7zwggYMGKCamhpJ0pdffqmZM2dq3759OuOMMzRy5Ei99dZb7rAEAACA+NFWQYIhQwe/PqiFoxZqxQcrWi2jWzp2KSUJ6BQWwzACdxrGCYfDoYyMDDU2NspqtUZ7OgAAAKbhXZJQ66jVj9f+uM1xL0x5QVPPm9qqYIE7QmivYLNBTBYoAAAAID74KknwrsT2J7tHtpKTkilJQNQQhgAAABCWlpIE771BDccaAo6zyCKb1UZBAqIuJtvkAAAAENsClSSczrsxjoIExBLCEAAAAELWVklCC+8lczarTaunrqYgATGBZXIAAABok6+ShGAsKV6iXGsuBQmISYQhAAAABNSekoRcay4FCYhZhCEAAAD4RUkCEhl7hgAAAOATJQlIdIQhAAAA+ERJAhIdy+QAAAAgSXI6pTfekOrqpOxsqfaMuqDGUZKAeEUYAgAAgNaskW67Tfr8tBtBfS7Olsa3PZaSBMQrlskBAACY3Jo10tVXewYhSWrYWiQ12iSvPUEtLLLIbrVTkoC4RRgCAAAwMafTdUfI8NWR0JwsVSyTDEoSkJgIQwAAACb2xhut7wh52D5FWrVafVJyPQ5TkoBEwJ4hAAAAE2lVklAbxKDtU7RkwETlXvoGJQlIKIQhAAAAk/BZktDH//mny81JpiQBCYcwBAAAYAItJQnee4MaGgKPs1gkm00qoiMBCYg9QwAAAAkuYEnCaSwW36+XLpWSWRGHBEQYAgAASHBtliT8m/eSOZtNWr1amkJHAhIUy+QAAAASTFglCZKWLJFyc0+NKyrijhASG2EIAAAggbSrJCFXGjUqItMCYhJhCAAAIEFQkgCEhj1DAAAACYCSBCB0hCEAAIAEQEkCEDqWyQEAAMQhShKA9iMMAQAAxBlKEoCOQRgCAACII5QkAB2HPUMAAABxgpIEoGMRhgAAAOIEJQlAx2KZHAAAQIyiJAGILMIQAABADKIkAYg8whAAAECMoSQB6BzsGQIAAIghlCQAnYcwBAAAEEMoSQA6D8vkAAAAosS7IKGoyPV9MChJANqPMAQAABAFvgoSbDZp5szgxlOSALSfxTDaWpEaHxwOhzIyMtTY2Cir1Rrt6QAAAPjlryDBYnEd691bOnTI976hlpKE6mruBAH+BJsN2DMEAADQiQIVJBiGZzECJQlAZLFMDgAAIIK89wU5nYELEgxDOnhQWrhQWrGi9TK6pUspSQA6CmEIAAAgQnztC+rVK7ix+flSTU3rggXuCAEdhzAEAAAQAf72BR06FNz47GxX8KEkAYgcwhAAAEAHC/bBqb60FCQUFXX8vAB4okABAACggwX74FRvFCQAnYs7QwAAAO3kXZJQWxvcuF69PJfNUZAAdC7CEAAAQDv4Kkno0ye4satWue4AUZAARAdhCAAAIEz+ShIaGgKPa9kXNGoU4QeIJvYMAQAAhCHYkgQenArELsIQAABAGIItSfBeMmezSatXsy8IiAUskwMAAAhCuCUJS5ZIubnsCwJiEWEIAACgDe0pScjN5cGpQKwiDAEAAATQ3pIEHp4KxC72DAEAAPhBSQKQ2AhDAAAAflCSACQ2lskBAAD8GyUJgLkQhgAAAERJAmBGhCEAAGB6lCQA5sSeIQAAYGqUJADmRRgCAACmRkkCYF4skwMAAKZCSQKAFoQhAABgGpQkADgdYQgAAJgCJQkAvLFnCAAAJDxKEgD4QhgCAAAJj5IEAL6wTA4AACQcShIABIMwBAAAEgolCQCCRRgCAAAJg5IEAKFgzxAAAEgIlCQACBVhCAAAJARKEgCEimVyAAAgLlGSAKC9CEMAACDuUJIAoCMQhgAAQFyhJAFARwl5z9CmTZs0YcIE5eTkyGKxaN26dQHPr6ur04wZM3T22WcrKSlJs2fP9nneyy+/rMGDBystLU1Dhw7Vq6++GurUAABAgqMkAUBHCjkMHT16VMOGDdPy5cuDOr+pqUl9+/bVvHnzNGzYMJ/nvPXWW5o+fbquv/56bdu2TZMmTdKkSZP00UcfhTo9AACQwChJANCRLIbR1n9bCTDYYtHatWs1adKkoM4fNWqUhg8frqVLl3ocnzZtmo4ePaq//e1v7mPf/va3NXz4cD366KNBvbfD4VBGRoYaGxtltVqD/QgAACCG+SpJ+PGP2x733HOUJABmFmw2iIk9Q1u2bFFpaanHseLi4oBL8JqamtTU1OR+7XA4IjU9AAAQBZQkAIi0mHjO0L59+5SZmelxLDMzU/v27fM7ZtGiRcrIyHB/2e32SE8TAAB0kpaSBO8lccGUJNjtlCQACE5MhKFwzJ07V42Nje6vvXv3RntKAACgA1CSAKCzxMQyuaysLNXX13scq6+vV1ZWlt8xqampSk1NjfTUAABAJwulJOHAgVOvbTZXEKIkAUCwYiIMFRYWqrKy0qN2e/369SosLIzepAAAQMR5FyQUFbm+D8aSJZQkAGifkMPQkSNHtGvXLvfr6upqVVVVqVevXurfv7/mzp2r2tpaPfPMM+5zqqqq3GMPHDigqqoqpaSk6Nxzz5Uk3Xbbbbriiiv0wAMPaPz48XrppZf0/vvv6/HHH2/nxwMAALHKV0GCzSbNnBnceEoSALRXyNXaGzdu1OjRo1sdLykpUXl5uX7yk5+opqZGGzduPPVLvBf1ShowYIBqamrcr19++WXNmzdPNTU1ys/P1+9//3uNGzcu6HlRrQ0AQPxoKUjw/luIxeI61ru3dOiQ731DFosrNFVXcycIgG/BZoN2PWcolhCGAACID06nlJfnf1+QxSL16uUKQ5JnIGr576s8QBVAIMFmg7htkwMAAPGprYIEw5AOHpQWLHAthTudzUYQAtBxYqJAAQAAJC7vkoTa2uDG5edLNTWtCxZYGgegoxCGAABAxPgqSejTJ7ix2dmu4ENJAoBIIQwBAICI8FeS0NAQeFxLQUJRUeTmBgASe4YAAEAEOJ2uO0Jt1TR5F862vF66lOVwACKPMAQAADpcWyUJLbyXzFGQAKAzsUwOAAC0W7glCUuWuBrjKEgAEA2EIQAA0C7tKUnIzaUgAUD0EIYAAEDYKEkAEM/YMwQAAMJCSQKAeEcYAgAAYaEkAUC8Y5kcAAAICiUJABINYQgAALSJkgQAiYgwBAAAAqIkAUCiYs8QAADwi5IEAImMMAQAAPyiJAFAImOZHAAAcKMkAYCZEIYAAIAkShIAmA9hCAAAUJIAwJTYMwQAgMlRkgDArAhDAACYHCUJAMyKZXIAAJgMJQkA4EIYAgDARChJAIBTCEMAAJgEJQkA4Ik9QwAAmAAlCQDQGmEIAAAToCQBAFpjmRwAAAmIkgQAaBthCACABENJAgAEhzAEAEACoSQBAILHniEAABIEJQkAEBrCEAAACYKSBAAIDcvkAACIU5QkAED7EIYAAIhDlCQAQPsRhgAAiDOUJABAx2DPEAAAcYSSBADoOIQhAADiCCUJANBxWCYHAECM8i5IKCpyfR8MShIAoG2EIQAAYpCvggSbTZo5M7jxlCQAQNsshtHWquP44HA4lJGRocbGRlmt1mhPBwCAsPkrSLBYXMd695YOHfK9b6ilJKG6mjtBAMwr2GzAniEAAGJIoIIEw/AsRqAkAQDahzAEAEAMaasgwTCkgwelBQtcS+FOR0kCAISGPUMAAESRd0lCbW1w4/LzpZqa1gUL3BECgOARhgAAiBJfJQneldj+ZGe7gg8lCQAQPsIQAABR4K8koaEh8LiWgoSiosjNDQDMgj1DAAB0skAlCaejIAEAIoswBABAJ2urJKGF95I5ChIAoGOxTA4AgAgLtyRhyRJXYxwFCQAQGYQhAAAiqD0lCbm5FCQAQCQRhgAAiBBKEgAgtrFnCACACKAkAQBiH2EIAIAIoCQBAGIfy+QAAOgAlCQAQPwhDAEA0E6UJABAfCIMAQDQDpQkAED8Ys8QAABhoiQBAOIbd4YAAAiS974gpzP4koQDB069ttlcQYiSBACILsIQAABB8LUvqFev4MZSkgAAsYkwBABAG/ztCzp0KLjxlCQAQGwiDAEAEECw+4J8oSQBAGIbBQoAAAQQ7MNTvVGSAACxjztDAACcJtyHp/bq5blsjpIEAIh9hCEAAP6tPQ9PXbXKdQeIkgQAiB+EIQAA1P6Hp44aRfgBgHjDniEAgOnx8FQAMCfCEADA9IItSfBeMmezSatXsy8IAOIVy+QAAKYTbkkCD08FgMRCGAIAmEp7ShJ4eCoAJBbCEADANNpbksDDUwEgsbBnCABgCpQkAAC8EYYAAKZASQIAwBvL5AAACYmSBABAWwhDAICEQ0kCACAYIS+T27RpkyZMmKCcnBxZLBatW7euzTEbN27UiBEjlJqaqrPOOkvl5eUeP1+wYIEsFovH1+DBg0OdGgAA7pIE7yVxwZQk2O2UJACAmYQcho4ePaphw4Zp+fLlQZ1fXV2t8ePHa/To0aqqqtLs2bN1ww036LXXXvM477zzzlNdXZ37a/PmzaFODQBgcpQkAABCEfIyuauuukpXXXVV0Oc/+uijGjhwoB544AFJ0pAhQ7R582YtWbJExcXFpybSpYuysrKCft+mpiY1NTW5XzscjqDHAgASUyglCQcOnHpts7mCECUJAGAuEd8ztGXLFo0ZM8bjWHFxsWbPnu1x7JNPPlFOTo7S0tJUWFioRYsWqX///n7fd9GiRVq4cGEkpgwAiBOUJAAA2iPiYWjfvn3KzMz0OJaZmSmHw6Gvv/5a6enpKigoUHl5uc455xzV1dVp4cKFKioq0kcffaQePXr4fN+5c+eqtLTU/drhcMhut0f0swAAYgclCQCA9oqJNrnTl91dcMEFKigo0IABA7Rq1Spdf/31PsekpqYqNTW1s6YIAIghLSUJ3nuDgilJsNkoSQAAuET8oatZWVmqr6/3OFZfXy+r1ar09HSfY3r27Kmzzz5bu3btivT0AABxhpIEAEBHiXgYKiwsVGVlpcex9evXq7Cw0O+YI0eO6NNPP1V2dnakpwcAiDOhlCSczmaTVq+mJAEAcErIy+SOHDniccemurpaVVVV6tWrl/r376+5c+eqtrZWzzzzjCTppptu0sMPP6w777xTP/3pT/XPf/5Tq1at0iuvvOJ+jzvuuEMTJkzQgAED9MUXX6isrEzJycmaPn16B3xEAEC88i5IKCpyfR8MShIAAG0JOQy9//77Gj16tPt1S4lBSUmJysvLVVdXpz179rh/PnDgQL3yyiu6/fbbtWzZMtlsNj3xxBMetdqff/65pk+froMHD6pv37667LLL9Pbbb6tv377t+WwAgDjmqyDBZpNmzgxuPCUJAIC2WAyjrVXX8cHhcCgjI0ONjY2yWq3Rng4AoB38FSRYLK5jvXtLhw753jfUUpJQXc2dIAAwq2CzQcT3DAEAEIpABQmG4VmMQEkCAKA9CEMAgJjSVkGCYUgHD0oLFriWwp2OkgQAQChi4jlDAADz8i5JqK0Nblx+vlRT07pggTtCAIBgEYYAAFHjqyTBuxLbn+xsV/ChJAEAEC7CEAAgKvyVJDQ0BB7XUpBQVBS5uQEAzIE9QwCATheoJOF0FCQAACKJMAQA6HRtlSS08F4yR0ECAKAjsUwOABBx4ZYkLFniaoyjIAEAEAmEIQBARLWnJCE3l4IEAEDkEIYAABFDSQIAIJaxZwgAEBGUJAAAYh1hCAAQEZQkAABiHcvkAAAdgpIEAEC8IQwBANqNkgQAQDwiDAEA2oWSBABAvGLPEAAgbJQkAADiGWEIABA2ShIAAPGMZXIAgKBRkgAASCSEIQBAUChJAAAkGsIQAKBNlCQAABIRe4YAAAFRkgAASFSEIQBAQJQkAAASFcvkAAAeKEkAAJgFYQgA4EZJAgDATAhDAABJlCQAAMyHPUMAAEoSAACmRBgCAFCSAAAwJZbJAYAJUZIAAABhCABMh5IEAABcCEMAYCKUJAAAcAp7hgDAJChJAADAE2EIAEyCkgQAADyxTA4AEhQlCQAABEYYAoAEREkCAABtIwwBQIKhJAEAgOCwZwgAEgglCQAABI8wBAAJhJIEAACCxzI5AIhT3gUJRUWu74NBSQIAAIQhAIhLvgoSbDZp5szgxlOSAACAZDGMtlaWxweHw6GMjAw1NjbKarVGezoAEDH+ChIsFtex3r2lQ4d87xtqKUmoruZOEAAgcQWbDdgzBABxJFBBgmF4FiNQkgAAQGCEIQCII20VJBiGdPCgtGCBaync6ShJAADAE3uGACCGeZck1NYGNy4/X6qpaV2wwB0hAABOIQwBQIzyVZLgXYntT3a2K/hQkgAAgH+EIQCIQf5KEhoaAo9rKUgoKorc3AAASBTsGQKAGBOoJOF0FCQAANA+hCEAiDFtlSS08F4yR0ECAAChYZkcAERZuCUJS5a4GuMoSAAAIDyEIQCIovaUJOTmUpAAAEB7EIYAIEooSQAAILrYMwQAUUBJAgAA0UcYAoAooCQBAIDoY5kcAHQCShIAAIg9hCEAiDBKEgAAiE2EIQCIIEoSAACIXewZAoAIoSQBAIDYRhgCgAihJAEAgNjGMjkA6CCUJAAAEF8IQwDQAShJAAAg/hCGAKCdKEkAACA+sWcIANqBkgQAAOIXYQgA2oGSBAAA4hfL5AAgBJQkAACQOAhDABAkShIAAEgshCEACAIlCQAAJB72DAFAGyhJAAAgMXFnCAC8eO8LcjqDL0k4cODUa5vNFYQoSQAAIDYRhgDgNL72BfXqFdxYShIAAIgvhCEA+Dd/+4IOHQpuPCUJAADEF8IQACj4fUG+UJIAAEB8okABABT8w1O9UZIAAED8CjkMbdq0SRMmTFBOTo4sFovWrVvX5piNGzdqxIgRSk1N1VlnnaXy8vJW5yxfvlx5eXlKS0tTQUGB3n333VCnBgBBczqljRulF190/TPYh6d67x+y2aTVqylJAAAgHoW8TO7o0aMaNmyYfvrTn2pKEP/vX11drfHjx+umm27S888/r8rKSt1www3Kzs5WcXGxJGnlypUqLS3Vo48+qoKCAi1dulTFxcXauXOn+vXrF/qnAoAA2vPw1FWrXHeAKEkAACD+WQwjnBXy/x5ssWjt2rWaNGmS33PuuusuvfLKK/roo4/cx374wx/qq6++UkVFhSSpoKBAF198sR5++GFJUnNzs+x2u2655RbNmTMnqLk4HA5lZGSosbFRVqs13I8EIMH5K0loS8u+oOpqwg8AALEu2GwQ8T1DW7Zs0ZgxYzyOFRcXa8uWLZKk48ePa+vWrR7nJCUlacyYMe5zfGlqapLD4fD4AoBAeHgqAAA4XcTD0L59+5SZmelxLDMzUw6HQ19//bUaGhrkdDp9nrNv3z6/77to0SJlZGS4v+x2e0TmDyBxBFuS4L1kjn1BAAAkprit1p47d65KS0vdrx0OB4EIgAen0xWAWvb3BFuSwMNTAQAwh4iHoaysLNXX13scq6+vl9VqVXp6upKTk5WcnOzznKysLL/vm5qaqtTU1IjMGUD8a09JAg9PBQDAHCK+TK6wsFCVlZUex9avX6/CwkJJUkpKikaOHOlxTnNzsyorK93nAEAoWkoSvJfENTQEHmexSHY7D08FAMAsQg5DR44cUVVVlaqqqiS5qrOrqqq0Z88eSa7la9dee637/Jtuukm7d+/WnXfeqR07duiRRx7RqlWrdPvtt7vPKS0t1YoVK/T0009r+/bt+vnPf66jR4/quuuua+fHA2A2lCQAAIBghbxM7v3339fo0aPdr1v27ZSUlKi8vFx1dXXuYCRJAwcO1CuvvKLbb79dy5Ytk81m0xNPPOF+xpAkTZs2TQcOHND8+fO1b98+DR8+XBUVFa1KFQCgLaGUJBw4cOq1zeYKQpQkAABgHu16zlAs4TlDgPl4FyQUFbkeijpjRttjn3uOkgQAABJVsNkgbtvkAJibr4IEm02aOTO48ZQkAAAA7gwBiDstBQnef3pZLK5jvXtLhw753jdksbhCU3U1d4IAAEhUwWaDiLfJAUBHClSQYBiexQiUJAAAgEAIQwDiSlsFCYYhHTwoLVjgWgp3OptNWr2akgQAAODCniEAMc27JKG2Nrhx+flSTU3rggXuCAEAgBaEIQAxy1dJQp8+wY3NznYFH0oSAACAP4QhADHJX0lCQ0PgcS0FCUVFkZsbAABIDOwZAhBzApUknI6CBAAA0B6EIQAxp62ShBbeS+YoSAAAAKFgmRyAqAu3JGHJEldjHAUJAAAgHIQhAFHVnpKE3FwKEgAAQPgIQwCihpIEAAAQTewZAhAVlCQAAIBoIwwBiApKEgAAQLSxTA5Ap6AkAQAAxBrCEICIoyQBAADEIsIQgIiiJAEAAMQq9gwBiBhKEgAAQCwjDAGIGEoSAABALGOZHIAOQ0kCAACIJ4QhAB2CkgQAABBvCEMA2o2SBAAAEI/YMwSgXShJAAAA8YowBKBdKEkAAADximVyAEJCSQIAAEgUhCEAQaMkAQAAJBLCEICgUJIAAAASDXuGALSJkgQAAJCICEMA2kRJAgAASEQskwPQCiUJAADADAhDADxQkgAAAMyCMATAjZIEAABgJuwZAiCJkgQAAGA+hCEAkihJAAAA5sMyOcCkKEkAAABmRxgCTIiSBAAAAMIQYDqUJAAAALiwZwgwEUoSAAAATiEMASZCSQIAAMApLJMDEhglCQAAAP4RhoAERUkCAABAYIQhIAFRkgAAANA29gwBCYaSBAAAgOAQhoAEQ0kCAABAcFgmB8Qx74KEoiLX98GgJAEAAJgdYQiIU74KEmw2aebM4MZTkgAAAMzOYhht7SyIDw6HQxkZGWpsbJTVao32dICI8leQYLG4jvXuLR065HvfUEtJQnU1d4IAAEBiCjYbsGcIiDOBChIMw7MYgZIEAAAA/whDQJxpqyDBMKSDB6UFC1xL4U5HSQIAAMAp7BkCYpx3SUJtbXDj8vOlmprWBQvcEQIAAHAhDAExzFdJgncltj/Z2a7gQ0kCAACAb4QhIEb5K0loaAg8rqUgoagocnMDAABIBOwZAmJQoJKE01GQAAAAED7CEBCD2ipJaOG9ZI6CBAAAgOCxTA6IAeGWJCxZ4mqMoyABAAAgdIQhIMraU5KQm0tBAgAAQLgIQ0AUUZIAAAAQPewZAqKEkgQAAIDoIgwBUUJJAgAAQHSxTA7oJJQkAAAAxBbCENAJKEkAAACIPYQhIMIoSQAAAIhN7BkCIoiSBAAAgNhFGAIiiJIEAACA2MUyOaADUZIAAAAQPwhDQAehJAEAACC+EIaADkBJAgAAQPxhzxDQTpQkAAAAxCfCENBOlCQAAADEJ5bJASGiJAEAACAxEIaAEFCSAAAAkDgIQ0CQKEkAAABILGHtGVq+fLny8vKUlpamgoICvfvuu37PPXHihO655x4NGjRIaWlpGjZsmCoqKjzOWbBggSwWi8fX4MGDw5kaEBGUJAAAACSekMPQypUrVVpaqrKyMn3wwQcaNmyYiouLtX//fp/nz5s3T4899pgeeughffzxx7rppps0efJkbdu2zeO88847T3V1de6vzZs3h/eJgAigJAEAACDxWAyjrf/W7amgoEAXX3yxHn74YUlSc3Oz7Ha7brnlFs2ZM6fV+Tk5Obr77rs1a9Ys97Ef/OAHSk9P13PPPSfJdWdo3bp1qqqqCnoeTU1Nampqcr92OByy2+1qbGyU1WoN5SMBrfgqSfjxj9se99xzlCQAAABEm8PhUEZGRpvZIKQ9Q8ePH9fWrVs1d+5c97GkpCSNGTNGW7Zs8TmmqalJaWlpHsfS09Nb3fn55JNPlJOTo7S0NBUWFmrRokXq37+/37ksWrRICxcuDGX6QFAoSQAAADCHkJbJNTQ0yOl0KjMz0+N4Zmam9u3b53NMcXGxHnzwQX3yySdqbm7W+vXrtWbNGtXV1bnPKSgoUHl5uSoqKvTHP/5R1dXVKioq0uHDh/3OZe7cuWpsbHR/7d27N5SPAvjUUpLgvSQumJIEu52SBAAAgHgS8Ta5ZcuWaebMmRo8eLAsFosGDRqk6667Tk8++aT7nKuuusr9/QUXXKCCggINGDBAq1at0vXXX+/zfVNTU5Wamhrp6cNEQilJOP0cShIAAADiU0h3hvr06aPk5GTV19d7HK+vr1dWVpbPMX379tW6det09OhRffbZZ9qxY4e6d++uM8880+/v6dmzp84++2zt2rUrlOkB7UJJAgAAgLmEFIZSUlI0cuRIVVZWuo81NzersrJShYWFAcempaUpNzdXJ0+e1J///GdNnDjR77lHjhzRp59+quzs7FCmB4TE6ZQ2bpRefNH1z9ra4MYtWSJt2CC98ILrn9XVBCEAAIB4FPIyudLSUpWUlOiiiy7SJZdcoqVLl+ro0aO67rrrJEnXXnutcnNztWjRIknSO++8o9raWg0fPly1tbVasGCBmpubdeedd7rf84477tCECRM0YMAAffHFFyorK1NycrKmT5/eQR8T8ERJAgAAAEIOQ9OmTdOBAwc0f/587du3T8OHD1dFRYW7VGHPnj1KSjp1w+mbb77RvHnztHv3bnXv3l3jxo3Ts88+q549e7rP+fzzzzV9+nQdPHhQffv21WWXXaa3335bffv2bf8nBLy0lCR47w0KpiTBZqMkAQAAIFGE/JyhWBVslzjMzemU8vLa3hvkrySBvUEAAACxL9hsENKeISCeeO8JanmQKiUJAAAAkDqhWhuIBl97gmw21/K4YCxZ4tobVFcnZWe7lsZRmw0AAJBYCENIOP72BNXWup4FFAxKEgAAABIfy+SQUAI9OLXlWHLyqT1A3iwWyW6nJAEAAMAMCENIKMHsCXI6XcHIOxC1vF66lCVxAAAAZkAYQlwL98Gps2e7lsKdjpIEAAAAc2HPEOJWex6cOnGitHix604SJQkAAADmRBhCXOqIB6cmJ1OSAAAAYGYsk0PcCVSScDr2BAEAACAQwhDiDg9OBQAAQEdgmRxintPpubcn2JIEHpwKAACAQAhDiGntKUngwakAAAAIhDCEmNURJQkAAACAP+wZQkyiJAEAAACRRhhCTKIkAQAAAJHGMjnEBEoSAAAA0NkIQ4g6ShIAAAAQDYQhRBUlCQAAAIgW9gwhaihJAAAAQDQRhhA1lCQAAAAgmlgmh05DSQIAAABiCWEInYKSBAAAAMQawhAijpIEAAAAxCL2DCGiKEkAAABArCIMIaIoSQAAAECsYpkcOhQlCQAAAIgXhCF0GEoSAAAAEE8IQ+gQlCQAAAAg3rBnCO1GSQIAAADiEWEI7UZJAgAAAOIRy+QQMkoSAAAAkAgIQwgJJQkAAABIFIQhBI2SBAAAACQS9gwhKJQkAAAAINEQhhAUShIAAACQaFgmB58oSQAAAECiIwyhFUoSAAAAYAaEIXigJAEAAABmwZ4huFGSAAAAADMhDMGNkgQAAACYCcvkTIySBAAAAJgZYcikKEkAAACA2RGGTIiSBAAAAIA9Q6ZDSQIAAADgQhgyGUoSAAAAABeWySUw74KEoiLX98GgJAEAAACJjjCUoHwVJNhs0syZwY2nJAEAAACJzmIYbe0eiQ8Oh0MZGRlqbGyU1WqN9nSiyl9BgsXiOta7t3TokO99Qy0lCdXV3AkCAABAfAo2G7BnKMEEKkgwDM9iBEoSAAAAYGaEoQTTVkGCYUgHD0oLFriWwp2OkgQAAACYCXuG4px3SUJtbXDj8vOlmprWBQvcEQIAAIBZEIbimK+SBO9KbH+ys13Bh5IEAAAAmBVhKE75K0loaAg8rqUgoagocnMDAAAA4gF7huJQoJKE01GQAAAAAPhHGIpDbZUktPBeMkdBAgAAAHAKy+TiQLglCUuWuBrjKEgAAAAAWiMMxbj2lCTk5lKQAAAAAPhDGIphlCQAAAAAkcOeoRhFSQIAAAAQWYShGEVJAgAAABBZLJOLEZQkAAAAAJ2LMBQDKEkAAAAAOh9hKMooSQAAAACigz1DUURJAgAAABA9hKEooiQBAAAAiB6WyXUiShIAAACA2EEY6iSUJAAAAACxhTDUCShJAAAAAGIPe4YijJIEAAAAIDYRhiKMkgQAAAAgNrFMroNRkgAAAADEh7DuDC1fvlx5eXlKS0tTQUGB3n33Xb/nnjhxQvfcc48GDRqktLQ0DRs2TBUVFe16z1i1Zo2UlyeNHi3NmOH65+zZwY1tKUmYPt31T4IQAAAAEFkhh6GVK1eqtLRUZWVl+uCDDzRs2DAVFxdr//79Ps+fN2+eHnvsMT300EP6+OOPddNNN2ny5Mnatm1b2O8Zi1pKEryXxAVTkmC3U5IAAAAAdDaLYbS1td9TQUGBLr74Yj388MOSpObmZtntdt1yyy2aM2dOq/NzcnJ09913a9asWe5jP/jBD5Senq7nnnsurPf0xeFwKCMjQ42NjbJaraF8pHZzOl13hNraG2SxeBYptJQksDcIAAAA6DjBZoOQ7gwdP35cW7du1ZgxY069QVKSxowZoy1btvgc09TUpLS0NI9j6enp2rx5c9jv2fK+DofD4ytaKEkAAAAA4k9IBQoNDQ1yOp3KzMz0OJ6ZmakdO3b4HFNcXKwHH3xQl19+uQYNGqTKykqtWbNGTqcz7PeUpEWLFmnhwoWhTD9i6uqCO4+SBAAAACB2RLxae9myZcrPz9fgwYOVkpKim2++Wdddd52Sktr3q+fOnavGxkb31969eztoxqHLzg7uPEoSAAAAgNgRUiLp06ePkpOTVV9f73G8vr5eWVlZPsf07dtX69at09GjR/XZZ59px44d6t69u84888yw31OSUlNTZbVaPb6ipajIteTN+8GpLShJAAAAAGJPSGEoJSVFI0eOVGVlpftYc3OzKisrVVhYGHBsWlqacnNzdfLkSf35z3/WxIkT2/2esSI5WVq2zPW9dyBqeb10KXeCAAAAgFgS8lq10tJSrVixQk8//bS2b9+un//85zp69Kiuu+46SdK1116ruXPnus9/5513tGbNGu3evVtvvPGGxo4dq+bmZt15551Bv2c8mDLFVYaQm+t5nJIEAAAAIDaFVKAgSdOmTdOBAwc0f/587du3T8OHD1dFRYW7AGHPnj0e+4G++eYbzZs3T7t371b37t01btw4Pfvss+rZs2fQ7xkvpkyRJk50tctRkgAAAADEtpCfMxSrovmcIQAAAACxIyLPGQIAAACAREEYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKhCEAAAAApkQYAgAAAGBKXaI9gY5iGIYkyeFwRHkmAAAAAKKpJRO0ZAR/EiYMHT58WJJkt9ujPBMAAAAAseDw4cPKyMjw+3OL0VZcihPNzc364osv1KNHD1kslqjOxeFwyG63a+/evbJarVGdC+IH1w3CwXWDcHHtIBxcNwhHNK4bwzB0+PBh5eTkKCnJ/86ghLkzlJSUJJvNFu1peLBarfxBgZBx3SAcXDcIF9cOwsF1g3B09nUT6I5QCwoUAAAAAJgSYQgAAACAKRGGIiA1NVVlZWVKTU2N9lQQR7huEA6uG4SLawfh4LpBOGL5ukmYAgUAAAAACAV3hgAAAACYEmEIAAAAgCkRhgAAAACYEmEIAAAAgCkRhgAAAACYEmEoTMuXL1deXp7S0tJUUFCgd999N+D5L7/8sgYPHqy0tDQNHTpUr776aifNFLEklOtmxYoVKioq0hlnnKEzzjhDY8aMafM6Q2IK9c+bFi+99JIsFosmTZoU2QkiZoV67Xz11VeaNWuWsrOzlZqaqrPPPpv/vzKhUK+bpUuX6pxzzlF6errsdrtuv/12ffPNN500W8SCTZs2acKECcrJyZHFYtG6devaHLNx40aNGDFCqampOuuss1ReXh7xefpCGArDypUrVVpaqrKyMn3wwQcaNmyYiouLtX//fp/nv/XWW5o+fbquv/56bdu2TZMmTdKkSZP00UcfdfLMEU2hXjcbN27U9OnTtWHDBm3ZskV2u13f+973VFtb28kzRzSFet20qKmp0R133KGioqJOmiliTajXzvHjx3XllVeqpqZGq1ev1s6dO7VixQrl5uZ28swRTaFeNy+88ILmzJmjsrIybd++XX/605+0cuVK/frXv+7kmSOajh49qmHDhmn58uVBnV9dXa3x48dr9OjRqqqq0uzZs3XDDTfotddei/BMfTAQsksuucSYNWuW+7XT6TRycnKMRYsW+Tx/6tSpxvjx4z2OFRQUGD/72c8iOk/EllCvG28nT540evToYTz99NORmiJiUDjXzcmTJ41LL73UeOKJJ4ySkhJj4sSJnTBTxJpQr50//vGPxplnnmkcP368s6aIGBTqdTNr1izju9/9rsex0tJS4zvf+U5E54nYJclYu3ZtwHPuvPNO47zzzvM4Nm3aNKO4uDiCM/ONO0MhOn78uLZu3aoxY8a4jyUlJWnMmDHasmWLzzFbtmzxOF+SiouL/Z6PxBPOdePt2LFjOnHihHr16hWpaSLGhHvd3HPPPerXr5+uv/76zpgmYlA4185f//pXFRYWatasWcrMzNT555+v3/3ud3I6nZ01bURZONfNpZdeqq1bt7qX0u3evVuvvvqqxo0b1ylzRnyKpb8bd+n03xjnGhoa5HQ6lZmZ6XE8MzNTO3bs8Dlm3759Ps/ft29fxOaJ2BLOdePtrrvuUk5OTqs/PJC4wrluNm/erD/96U+qqqrqhBkiVoVz7ezevVv//Oc/9aMf/Uivvvqqdu3apV/84hc6ceKEysrKOmPaiLJwrpsZM2aooaFBl112mQzD0MmTJ3XTTTexTA4B+fu7scPh0Ndff6309PROmwt3hoA4cN999+mll17S2rVrlZaWFu3pIEYdPnxY11xzjVasWKE+ffpEezqIM83NzerXr58ef/xxjRw5UtOmTdPdd9+tRx99NNpTQwzbuHGjfve73+mRRx7RBx98oDVr1uiVV17RvffeG+2pAUHhzlCI+vTpo+TkZNXX13scr6+vV1ZWls8xWVlZIZ2PxBPOddNi8eLFuu+++/T666/rggsuiOQ0EWNCvW4+/fRT1dTUaMKECe5jzc3NkqQuXbpo586dGjRoUGQnjZgQzp852dnZ6tq1q5KTk93HhgwZon379un48eNKSUmJ6JwRfeFcN//1X/+la665RjfccIMkaejQoTp69KhuvPFG3X333UpK4r+7ozV/fze2Wq2deldI4s5QyFJSUjRy5EhVVla6jzU3N6uyslKFhYU+xxQWFnqcL0nr16/3ez4STzjXjST9/ve/17333quKigpddNFFnTFVxJBQr5vBgwfrww8/VFVVlfvrP//zP91tPXa7vTOnjygK58+c73znO9q1a5c7QEvSv/71L2VnZxOETCKc6+bYsWOtAk9LoDYMI3KTRVyLqb8bd3plQwJ46aWXjNTUVKO8vNz4+OOPjRtvvNHo2bOnsW/fPsMwDOOaa64x5syZ4z7/zTffNLp06WIsXrzY2L59u1FWVmZ07drV+PDDD6P1ERAFoV439913n5GSkmKsXr3aqKurc38dPnw4Wh8BURDqdeONNjnzCvXa2bNnj9GjRw/j5ptvNnbu3Gn87W9/M/r162f85je/idZHQBSEet2UlZUZPXr0MF588UVj9+7dxj/+8Q9j0KBBxtSpU6P1ERAFhw8fNrZt22Zs27bNkGQ8+OCDxrZt24zPPvvMMAzDmDNnjnHNNde4z9+9e7fRrVs341e/+pWxfft2Y/ny5UZycrJRUVHR6XMnDIXpoYceMvr372+kpKQYl1xyifH222+7f3bFFVcYJSUlHuevWrXKOPvss42UlBTjvPPOM1555ZVOnjFiQSjXzYABAwxJrb7Kyso6f+KIqlD/vDkdYcjcQr123nrrLaOgoMBITU01zjzzTOO3v/2tcfLkyU6eNaItlOvmxIkTxoIFC4xBgwYZaWlpht1uN37xi18YX375ZedPHFGzYcMGn39nablWSkpKjCuuuKLVmOHDhxspKSnGmWeeaTz11FOdPm/DMAyLYXAPEwAAAID5sGcIAAAAgCkRhgAAAACYEmEIAAAAgCkRhgAAAACYEmEIAAAAgCkRhgAAAACYEmEIAAAAgCkRhgAAAACYEmEIAAAAgCkRhgAAAACYEmEIAAAAgCn9f2eVeov0TUlVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build a PyTorch model by subclassing nn.Module.\n",
    "\n",
    "1. Inside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\n",
    "2. Implement the forward() method to compute the linear regression function you used to create the dataset in 1.\n",
    "3. Once you've constructed the model, make an instance of it and check its state_dict().\n",
    "`Note: If you'd like to use nn.Linear() instead of nn.Parameter() you can.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.2259]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.9754], device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=1,out_features=1,dtype=torch.float,device=device)\n",
    "\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_0 = LinearRegressionModel()\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively.\n",
    "\n",
    "1. Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters\n",
    "from the model you created in 2.\n",
    "2. Write a training loop to perform the appropriate training steps for 300 epochs.\n",
    "3. The training loop should test the model on the test dataset every 20 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.to(device)\n",
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.04617559537291527 | Test Loss: 0.00778192887082696\n",
      "Epoch: 1 | Loss: 0.04501957818865776 | Test Loss: 0.00778192887082696\n",
      "Epoch: 2 | Loss: 0.04386356845498085 | Test Loss: 0.00778192887082696\n",
      "Epoch: 3 | Loss: 0.04270755872130394 | Test Loss: 0.00778192887082696\n",
      "Epoch: 4 | Loss: 0.041551556438207626 | Test Loss: 0.00778192887082696\n",
      "Epoch: 5 | Loss: 0.04039553180336952 | Test Loss: 0.00778192887082696\n",
      "Epoch: 6 | Loss: 0.03923952206969261 | Test Loss: 0.00778192887082696\n",
      "Epoch: 7 | Loss: 0.0380835086107254 | Test Loss: 0.00778192887082696\n",
      "Epoch: 8 | Loss: 0.03692749887704849 | Test Loss: 0.00778192887082696\n",
      "Epoch: 9 | Loss: 0.035771485418081284 | Test Loss: 0.00778192887082696\n",
      "Epoch: 10 | Loss: 0.034615471959114075 | Test Loss: 0.00778192887082696\n",
      "Epoch: 11 | Loss: 0.033459458500146866 | Test Loss: 0.00778192887082696\n",
      "Epoch: 12 | Loss: 0.032303448766469955 | Test Loss: 0.00778192887082696\n",
      "Epoch: 13 | Loss: 0.031151000410318375 | Test Loss: 0.00778192887082696\n",
      "Epoch: 14 | Loss: 0.03007500432431698 | Test Loss: 0.00778192887082696\n",
      "Epoch: 15 | Loss: 0.0290782880038023 | Test Loss: 0.00778192887082696\n",
      "Epoch: 16 | Loss: 0.0281821396201849 | Test Loss: 0.00778192887082696\n",
      "Epoch: 17 | Loss: 0.027351057156920433 | Test Loss: 0.00778192887082696\n",
      "Epoch: 18 | Loss: 0.02660883404314518 | Test Loss: 0.00778192887082696\n",
      "Epoch: 19 | Loss: 0.025920435786247253 | Test Loss: 0.00778192887082696\n",
      "Epoch: 20 | Loss: 0.025282863527536392 | Test Loss: 0.017358148470520973\n",
      "Epoch: 21 | Loss: 0.024693027138710022 | Test Loss: 0.017358148470520973\n",
      "Epoch: 22 | Loss: 0.0241480004042387 | Test Loss: 0.017358148470520973\n",
      "Epoch: 23 | Loss: 0.023644806817173958 | Test Loss: 0.017358148470520973\n",
      "Epoch: 24 | Loss: 0.023180542513728142 | Test Loss: 0.017358148470520973\n",
      "Epoch: 25 | Loss: 0.022752394899725914 | Test Loss: 0.017358148470520973\n",
      "Epoch: 26 | Loss: 0.022357525303959846 | Test Loss: 0.017358148470520973\n",
      "Epoch: 27 | Loss: 0.021993178874254227 | Test Loss: 0.017358148470520973\n",
      "Epoch: 28 | Loss: 0.021656600758433342 | Test Loss: 0.017358148470520973\n",
      "Epoch: 29 | Loss: 0.021348074078559875 | Test Loss: 0.017358148470520973\n",
      "Epoch: 30 | Loss: 0.02105153538286686 | Test Loss: 0.017358148470520973\n",
      "Epoch: 31 | Loss: 0.020777393132448196 | Test Loss: 0.017358148470520973\n",
      "Epoch: 32 | Loss: 0.020525125786662102 | Test Loss: 0.017358148470520973\n",
      "Epoch: 33 | Loss: 0.020283974707126617 | Test Loss: 0.017358148470520973\n",
      "Epoch: 34 | Loss: 0.020060045644640923 | Test Loss: 0.017358148470520973\n",
      "Epoch: 35 | Loss: 0.019857212901115417 | Test Loss: 0.017358148470520973\n",
      "Epoch: 36 | Loss: 0.019659431651234627 | Test Loss: 0.017358148470520973\n",
      "Epoch: 37 | Loss: 0.0194788109511137 | Test Loss: 0.017358148470520973\n",
      "Epoch: 38 | Loss: 0.01930389180779457 | Test Loss: 0.017358148470520973\n",
      "Epoch: 39 | Loss: 0.019143788143992424 | Test Loss: 0.017358148470520973\n",
      "Epoch: 40 | Loss: 0.01898840069770813 | Test Loss: 0.029131276533007622\n",
      "Epoch: 41 | Loss: 0.018847212195396423 | Test Loss: 0.029131276533007622\n",
      "Epoch: 42 | Loss: 0.018708160147070885 | Test Loss: 0.029131276533007622\n",
      "Epoch: 43 | Loss: 0.018584271892905235 | Test Loss: 0.029131276533007622\n",
      "Epoch: 44 | Loss: 0.018460387364029884 | Test Loss: 0.029131276533007622\n",
      "Epoch: 45 | Loss: 0.018342668190598488 | Test Loss: 0.029131276533007622\n",
      "Epoch: 46 | Loss: 0.018234487622976303 | Test Loss: 0.029131276533007622\n",
      "Epoch: 47 | Loss: 0.01812674105167389 | Test Loss: 0.029131276533007622\n",
      "Epoch: 48 | Loss: 0.01803271286189556 | Test Loss: 0.029131276533007622\n",
      "Epoch: 49 | Loss: 0.017938680946826935 | Test Loss: 0.029131276533007622\n",
      "Epoch: 50 | Loss: 0.01784464716911316 | Test Loss: 0.029131276533007622\n",
      "Epoch: 51 | Loss: 0.017757168039679527 | Test Loss: 0.029131276533007622\n",
      "Epoch: 52 | Loss: 0.01767572946846485 | Test Loss: 0.029131276533007622\n",
      "Epoch: 53 | Loss: 0.01759427972137928 | Test Loss: 0.029131276533007622\n",
      "Epoch: 54 | Loss: 0.017516562715172768 | Test Loss: 0.029131276533007622\n",
      "Epoch: 55 | Loss: 0.017446190118789673 | Test Loss: 0.029131276533007622\n",
      "Epoch: 56 | Loss: 0.017375817522406578 | Test Loss: 0.029131276533007622\n",
      "Epoch: 57 | Loss: 0.017305446788668633 | Test Loss: 0.029131276533007622\n",
      "Epoch: 58 | Loss: 0.017238914966583252 | Test Loss: 0.029131276533007622\n",
      "Epoch: 59 | Loss: 0.017178088426589966 | Test Loss: 0.029131276533007622\n",
      "Epoch: 60 | Loss: 0.01711726002395153 | Test Loss: 0.033673614263534546\n",
      "Epoch: 61 | Loss: 0.017056433483958244 | Test Loss: 0.033673614263534546\n",
      "Epoch: 62 | Loss: 0.016996657475829124 | Test Loss: 0.033673614263534546\n",
      "Epoch: 63 | Loss: 0.01694392040371895 | Test Loss: 0.033673614263534546\n",
      "Epoch: 64 | Loss: 0.01689113676548004 | Test Loss: 0.033673614263534546\n",
      "Epoch: 65 | Loss: 0.016838371753692627 | Test Loss: 0.033673614263534546\n",
      "Epoch: 66 | Loss: 0.016785606741905212 | Test Loss: 0.033673614263534546\n",
      "Epoch: 67 | Loss: 0.016732845455408096 | Test Loss: 0.033673614263534546\n",
      "Epoch: 68 | Loss: 0.0166828241199255 | Test Loss: 0.033673614263534546\n",
      "Epoch: 69 | Loss: 0.016636637970805168 | Test Loss: 0.033673614263534546\n",
      "Epoch: 70 | Loss: 0.01659044623374939 | Test Loss: 0.033673614263534546\n",
      "Epoch: 71 | Loss: 0.016544261947274208 | Test Loss: 0.033673614263534546\n",
      "Epoch: 72 | Loss: 0.016498073935508728 | Test Loss: 0.033673614263534546\n",
      "Epoch: 73 | Loss: 0.0164518803358078 | Test Loss: 0.033673614263534546\n",
      "Epoch: 74 | Loss: 0.01640591025352478 | Test Loss: 0.033673614263534546\n",
      "Epoch: 75 | Loss: 0.016364842653274536 | Test Loss: 0.033673614263534546\n",
      "Epoch: 76 | Loss: 0.01632377877831459 | Test Loss: 0.033673614263534546\n",
      "Epoch: 77 | Loss: 0.01628270372748375 | Test Loss: 0.033673614263534546\n",
      "Epoch: 78 | Loss: 0.016241634264588356 | Test Loss: 0.033673614263534546\n",
      "Epoch: 79 | Loss: 0.016200566664338112 | Test Loss: 0.033673614263534546\n",
      "Epoch: 80 | Loss: 0.016159500926733017 | Test Loss: 0.03497789055109024\n",
      "Epoch: 81 | Loss: 0.016118427738547325 | Test Loss: 0.03497789055109024\n",
      "Epoch: 82 | Loss: 0.01607736013829708 | Test Loss: 0.03497789055109024\n",
      "Epoch: 83 | Loss: 0.01603648066520691 | Test Loss: 0.03497789055109024\n",
      "Epoch: 84 | Loss: 0.015999099239706993 | Test Loss: 0.03497789055109024\n",
      "Epoch: 85 | Loss: 0.015961719676852226 | Test Loss: 0.03497789055109024\n",
      "Epoch: 86 | Loss: 0.01592433452606201 | Test Loss: 0.03497789055109024\n",
      "Epoch: 87 | Loss: 0.015886953100562096 | Test Loss: 0.03497789055109024\n",
      "Epoch: 88 | Loss: 0.01584957353770733 | Test Loss: 0.03497789055109024\n",
      "Epoch: 89 | Loss: 0.015812193974852562 | Test Loss: 0.03497789055109024\n",
      "Epoch: 90 | Loss: 0.015774814411997795 | Test Loss: 0.03497789055109024\n",
      "Epoch: 91 | Loss: 0.015737425535917282 | Test Loss: 0.03497789055109024\n",
      "Epoch: 92 | Loss: 0.015700051560997963 | Test Loss: 0.03497789055109024\n",
      "Epoch: 93 | Loss: 0.0156626645475626 | Test Loss: 0.03497789055109024\n",
      "Epoch: 94 | Loss: 0.015625283122062683 | Test Loss: 0.03497789055109024\n",
      "Epoch: 95 | Loss: 0.015587905421853065 | Test Loss: 0.03497789055109024\n",
      "Epoch: 96 | Loss: 0.015550563111901283 | Test Loss: 0.03497789055109024\n",
      "Epoch: 97 | Loss: 0.015515436418354511 | Test Loss: 0.03497789055109024\n",
      "Epoch: 98 | Loss: 0.015480316244065762 | Test Loss: 0.03497789055109024\n",
      "Epoch: 99 | Loss: 0.015445190481841564 | Test Loss: 0.03497789055109024\n",
      "Epoch: 100 | Loss: 0.015410062856972218 | Test Loss: 0.034733470529317856\n",
      "Epoch: 101 | Loss: 0.015374940820038319 | Test Loss: 0.034733470529317856\n",
      "Epoch: 102 | Loss: 0.015339813195168972 | Test Loss: 0.034733470529317856\n",
      "Epoch: 103 | Loss: 0.015304702334105968 | Test Loss: 0.034733470529317856\n",
      "Epoch: 104 | Loss: 0.015269565396010876 | Test Loss: 0.034733470529317856\n",
      "Epoch: 105 | Loss: 0.015234440565109253 | Test Loss: 0.034733470529317856\n",
      "Epoch: 106 | Loss: 0.01519931759685278 | Test Loss: 0.034733470529317856\n",
      "Epoch: 107 | Loss: 0.01516419555991888 | Test Loss: 0.034733470529317856\n",
      "Epoch: 108 | Loss: 0.015129068866372108 | Test Loss: 0.034733470529317856\n",
      "Epoch: 109 | Loss: 0.01509394496679306 | Test Loss: 0.034733470529317856\n",
      "Epoch: 110 | Loss: 0.015058818273246288 | Test Loss: 0.034733470529317856\n",
      "Epoch: 111 | Loss: 0.015023693442344666 | Test Loss: 0.034733470529317856\n",
      "Epoch: 112 | Loss: 0.014988568611443043 | Test Loss: 0.034733470529317856\n",
      "Epoch: 113 | Loss: 0.014953446574509144 | Test Loss: 0.034733470529317856\n",
      "Epoch: 114 | Loss: 0.014918318949639797 | Test Loss: 0.034733470529317856\n",
      "Epoch: 115 | Loss: 0.014883196912705898 | Test Loss: 0.034733470529317856\n",
      "Epoch: 116 | Loss: 0.014848071150481701 | Test Loss: 0.034733470529317856\n",
      "Epoch: 117 | Loss: 0.014812949113547802 | Test Loss: 0.034733470529317856\n",
      "Epoch: 118 | Loss: 0.01477782428264618 | Test Loss: 0.034733470529317856\n",
      "Epoch: 119 | Loss: 0.014742699451744556 | Test Loss: 0.034733470529317856\n",
      "Epoch: 120 | Loss: 0.014707580208778381 | Test Loss: 0.03389657661318779\n",
      "Epoch: 121 | Loss: 0.014673206023871899 | Test Loss: 0.03389657661318779\n",
      "Epoch: 122 | Loss: 0.014638935215771198 | Test Loss: 0.03389657661318779\n",
      "Epoch: 123 | Loss: 0.014604665338993073 | Test Loss: 0.03389657661318779\n",
      "Epoch: 124 | Loss: 0.014570394530892372 | Test Loss: 0.03389657661318779\n",
      "Epoch: 125 | Loss: 0.014536133967339993 | Test Loss: 0.03389657661318779\n",
      "Epoch: 126 | Loss: 0.014501864090561867 | Test Loss: 0.03389657661318779\n",
      "Epoch: 127 | Loss: 0.014467591419816017 | Test Loss: 0.03389657661318779\n",
      "Epoch: 128 | Loss: 0.014433324337005615 | Test Loss: 0.03389657661318779\n",
      "Epoch: 129 | Loss: 0.01439905446022749 | Test Loss: 0.03389657661318779\n",
      "Epoch: 130 | Loss: 0.014364788308739662 | Test Loss: 0.03389657661318779\n",
      "Epoch: 131 | Loss: 0.014330516569316387 | Test Loss: 0.03389657661318779\n",
      "Epoch: 132 | Loss: 0.014296247623860836 | Test Loss: 0.03389657661318779\n",
      "Epoch: 133 | Loss: 0.014261978678405285 | Test Loss: 0.03389657661318779\n",
      "Epoch: 134 | Loss: 0.014227712526917458 | Test Loss: 0.03389657661318779\n",
      "Epoch: 135 | Loss: 0.014193445444107056 | Test Loss: 0.03389657661318779\n",
      "Epoch: 136 | Loss: 0.014159171842038631 | Test Loss: 0.03389657661318779\n",
      "Epoch: 137 | Loss: 0.01412490475922823 | Test Loss: 0.03389657661318779\n",
      "Epoch: 138 | Loss: 0.01409063395112753 | Test Loss: 0.03389657661318779\n",
      "Epoch: 139 | Loss: 0.014056366868317127 | Test Loss: 0.03389657661318779\n",
      "Epoch: 140 | Loss: 0.014022103510797024 | Test Loss: 0.03236670419573784\n",
      "Epoch: 141 | Loss: 0.013987830840051174 | Test Loss: 0.03236670419573784\n",
      "Epoch: 142 | Loss: 0.013953560963273048 | Test Loss: 0.03236670419573784\n",
      "Epoch: 143 | Loss: 0.013919288292527199 | Test Loss: 0.03236670419573784\n",
      "Epoch: 144 | Loss: 0.013885023072361946 | Test Loss: 0.03236670419573784\n",
      "Epoch: 145 | Loss: 0.01385075319558382 | Test Loss: 0.03236670419573784\n",
      "Epoch: 146 | Loss: 0.013816478662192822 | Test Loss: 0.03236670419573784\n",
      "Epoch: 147 | Loss: 0.013782215304672718 | Test Loss: 0.03236670419573784\n",
      "Epoch: 148 | Loss: 0.013747951947152615 | Test Loss: 0.03236670419573784\n",
      "Epoch: 149 | Loss: 0.013713677413761616 | Test Loss: 0.03236670419573784\n",
      "Epoch: 150 | Loss: 0.013679416850209236 | Test Loss: 0.03236670419573784\n",
      "Epoch: 151 | Loss: 0.013645139522850513 | Test Loss: 0.03236670419573784\n",
      "Epoch: 152 | Loss: 0.013610872440040112 | Test Loss: 0.03236670419573784\n",
      "Epoch: 153 | Loss: 0.01357660349458456 | Test Loss: 0.03236670419573784\n",
      "Epoch: 154 | Loss: 0.01354233454912901 | Test Loss: 0.03236670419573784\n",
      "Epoch: 155 | Loss: 0.013508058153092861 | Test Loss: 0.03236670419573784\n",
      "Epoch: 156 | Loss: 0.013473793864250183 | Test Loss: 0.03236670419573784\n",
      "Epoch: 157 | Loss: 0.013439527712762356 | Test Loss: 0.03236670419573784\n",
      "Epoch: 158 | Loss: 0.01340525597333908 | Test Loss: 0.03236670419573784\n",
      "Epoch: 159 | Loss: 0.013370990753173828 | Test Loss: 0.03236670419573784\n",
      "Epoch: 160 | Loss: 0.013336730189621449 | Test Loss: 0.03083682619035244\n",
      "Epoch: 161 | Loss: 0.013302451930940151 | Test Loss: 0.03083682619035244\n",
      "Epoch: 162 | Loss: 0.013268186710774899 | Test Loss: 0.03083682619035244\n",
      "Epoch: 163 | Loss: 0.0132339121773839 | Test Loss: 0.03083682619035244\n",
      "Epoch: 164 | Loss: 0.013199645094573498 | Test Loss: 0.03083682619035244\n",
      "Epoch: 165 | Loss: 0.01316537894308567 | Test Loss: 0.03083682619035244\n",
      "Epoch: 166 | Loss: 0.013131109066307545 | Test Loss: 0.03083682619035244\n",
      "Epoch: 167 | Loss: 0.013096841983497143 | Test Loss: 0.03083682619035244\n",
      "Epoch: 168 | Loss: 0.013062569312751293 | Test Loss: 0.03083682619035244\n",
      "Epoch: 169 | Loss: 0.013028303161263466 | Test Loss: 0.03083682619035244\n",
      "Epoch: 170 | Loss: 0.012994018383324146 | Test Loss: 0.03083682619035244\n",
      "Epoch: 171 | Loss: 0.012959765270352364 | Test Loss: 0.03083682619035244\n",
      "Epoch: 172 | Loss: 0.012925497256219387 | Test Loss: 0.03083682619035244\n",
      "Epoch: 173 | Loss: 0.012891227379441261 | Test Loss: 0.03083682619035244\n",
      "Epoch: 174 | Loss: 0.012856957502663136 | Test Loss: 0.03083682619035244\n",
      "Epoch: 175 | Loss: 0.012822692282497883 | Test Loss: 0.03083682619035244\n",
      "Epoch: 176 | Loss: 0.012788421474397182 | Test Loss: 0.03083682619035244\n",
      "Epoch: 177 | Loss: 0.012754151597619057 | Test Loss: 0.03083682619035244\n",
      "Epoch: 178 | Loss: 0.01271988358348608 | Test Loss: 0.03083682619035244\n",
      "Epoch: 179 | Loss: 0.012685618363320827 | Test Loss: 0.03083682619035244\n",
      "Epoch: 180 | Loss: 0.012651339173316956 | Test Loss: 0.02930695377290249\n",
      "Epoch: 181 | Loss: 0.012617073953151703 | Test Loss: 0.02930695377290249\n",
      "Epoch: 182 | Loss: 0.012582805939018726 | Test Loss: 0.02930695377290249\n",
      "Epoch: 183 | Loss: 0.0125485360622406 | Test Loss: 0.02930695377290249\n",
      "Epoch: 184 | Loss: 0.012514273636043072 | Test Loss: 0.02930695377290249\n",
      "Epoch: 185 | Loss: 0.012480002827942371 | Test Loss: 0.02930695377290249\n",
      "Epoch: 186 | Loss: 0.012445732019841671 | Test Loss: 0.02930695377290249\n",
      "Epoch: 187 | Loss: 0.012411466799676418 | Test Loss: 0.02930695377290249\n",
      "Epoch: 188 | Loss: 0.012377193197607994 | Test Loss: 0.02930695377290249\n",
      "Epoch: 189 | Loss: 0.012342923320829868 | Test Loss: 0.02930695377290249\n",
      "Epoch: 190 | Loss: 0.012308653444051743 | Test Loss: 0.02930695377290249\n",
      "Epoch: 191 | Loss: 0.012274391017854214 | Test Loss: 0.02930695377290249\n",
      "Epoch: 192 | Loss: 0.012240123003721237 | Test Loss: 0.02930695377290249\n",
      "Epoch: 193 | Loss: 0.012205853126943111 | Test Loss: 0.02930695377290249\n",
      "Epoch: 194 | Loss: 0.012171579524874687 | Test Loss: 0.02930695377290249\n",
      "Epoch: 195 | Loss: 0.012137316167354584 | Test Loss: 0.02930695377290249\n",
      "Epoch: 196 | Loss: 0.012103044427931309 | Test Loss: 0.02930695377290249\n",
      "Epoch: 197 | Loss: 0.012068773619830608 | Test Loss: 0.02930695377290249\n",
      "Epoch: 198 | Loss: 0.012034508399665356 | Test Loss: 0.02930695377290249\n",
      "Epoch: 199 | Loss: 0.012000237591564655 | Test Loss: 0.02930695377290249\n",
      "Epoch: 200 | Loss: 0.011965969577431679 | Test Loss: 0.027777081355452538\n",
      "Epoch: 201 | Loss: 0.011931698769330978 | Test Loss: 0.027777081355452538\n",
      "Epoch: 202 | Loss: 0.011897428892552853 | Test Loss: 0.027777081355452538\n",
      "Epoch: 203 | Loss: 0.0118631636723876 | Test Loss: 0.027777081355452538\n",
      "Epoch: 204 | Loss: 0.0118288928642869 | Test Loss: 0.027777081355452538\n",
      "Epoch: 205 | Loss: 0.011794624850153923 | Test Loss: 0.027777081355452538\n",
      "Epoch: 206 | Loss: 0.011760354042053223 | Test Loss: 0.027777081355452538\n",
      "Epoch: 207 | Loss: 0.01172608695924282 | Test Loss: 0.027777081355452538\n",
      "Epoch: 208 | Loss: 0.011691821739077568 | Test Loss: 0.027777081355452538\n",
      "Epoch: 209 | Loss: 0.011657551862299442 | Test Loss: 0.027777081355452538\n",
      "Epoch: 210 | Loss: 0.011623281985521317 | Test Loss: 0.027777081355452538\n",
      "Epoch: 211 | Loss: 0.01158901583403349 | Test Loss: 0.027777081355452538\n",
      "Epoch: 212 | Loss: 0.01155474130064249 | Test Loss: 0.027777081355452538\n",
      "Epoch: 213 | Loss: 0.011520471423864365 | Test Loss: 0.027777081355452538\n",
      "Epoch: 214 | Loss: 0.011486206203699112 | Test Loss: 0.027777081355452538\n",
      "Epoch: 215 | Loss: 0.011451932601630688 | Test Loss: 0.027777081355452538\n",
      "Epoch: 216 | Loss: 0.011417663656175137 | Test Loss: 0.027777081355452538\n",
      "Epoch: 217 | Loss: 0.011383398436009884 | Test Loss: 0.027777081355452538\n",
      "Epoch: 218 | Loss: 0.011349127627909184 | Test Loss: 0.027777081355452538\n",
      "Epoch: 219 | Loss: 0.01131486427038908 | Test Loss: 0.027777081355452538\n",
      "Epoch: 220 | Loss: 0.011280592530965805 | Test Loss: 0.026247208938002586\n",
      "Epoch: 221 | Loss: 0.011246326379477978 | Test Loss: 0.026247208938002586\n",
      "Epoch: 222 | Loss: 0.011212055571377277 | Test Loss: 0.026247208938002586\n",
      "Epoch: 223 | Loss: 0.011177784763276577 | Test Loss: 0.026247208938002586\n",
      "Epoch: 224 | Loss: 0.0111435167491436 | Test Loss: 0.026247208938002586\n",
      "Epoch: 225 | Loss: 0.011109249666333199 | Test Loss: 0.026247208938002586\n",
      "Epoch: 226 | Loss: 0.011074974201619625 | Test Loss: 0.026247208938002586\n",
      "Epoch: 227 | Loss: 0.011040711775422096 | Test Loss: 0.026247208938002586\n",
      "Epoch: 228 | Loss: 0.011006440035998821 | Test Loss: 0.026247208938002586\n",
      "Epoch: 229 | Loss: 0.010972172021865845 | Test Loss: 0.026247208938002586\n",
      "Epoch: 230 | Loss: 0.010937904007732868 | Test Loss: 0.026247208938002586\n",
      "Epoch: 231 | Loss: 0.01090364158153534 | Test Loss: 0.026247208938002586\n",
      "Epoch: 232 | Loss: 0.010869362391531467 | Test Loss: 0.026247208938002586\n",
      "Epoch: 233 | Loss: 0.01083509624004364 | Test Loss: 0.026247208938002586\n",
      "Epoch: 234 | Loss: 0.010800832882523537 | Test Loss: 0.026247208938002586\n",
      "Epoch: 235 | Loss: 0.010766560211777687 | Test Loss: 0.026247208938002586\n",
      "Epoch: 236 | Loss: 0.01073228009045124 | Test Loss: 0.026247208938002586\n",
      "Epoch: 237 | Loss: 0.01069802325218916 | Test Loss: 0.026247208938002586\n",
      "Epoch: 238 | Loss: 0.010663751512765884 | Test Loss: 0.026247208938002586\n",
      "Epoch: 239 | Loss: 0.010629481635987759 | Test Loss: 0.026247208938002586\n",
      "Epoch: 240 | Loss: 0.01059521734714508 | Test Loss: 0.024717355147004128\n",
      "Epoch: 241 | Loss: 0.010560951195657253 | Test Loss: 0.024717355147004128\n",
      "Epoch: 242 | Loss: 0.010526677593588829 | Test Loss: 0.024717355147004128\n",
      "Epoch: 243 | Loss: 0.010492407716810703 | Test Loss: 0.024717355147004128\n",
      "Epoch: 244 | Loss: 0.010458140634000301 | Test Loss: 0.024717355147004128\n",
      "Epoch: 245 | Loss: 0.010423868894577026 | Test Loss: 0.024717355147004128\n",
      "Epoch: 246 | Loss: 0.010389617644250393 | Test Loss: 0.024717355147004128\n",
      "Epoch: 247 | Loss: 0.010355332866311073 | Test Loss: 0.024717355147004128\n",
      "Epoch: 248 | Loss: 0.010321064852178097 | Test Loss: 0.024717355147004128\n",
      "Epoch: 249 | Loss: 0.010286794044077396 | Test Loss: 0.024717355147004128\n",
      "Epoch: 250 | Loss: 0.010252526961266994 | Test Loss: 0.024717355147004128\n",
      "Epoch: 251 | Loss: 0.010218258947134018 | Test Loss: 0.024717355147004128\n",
      "Epoch: 252 | Loss: 0.010183989070355892 | Test Loss: 0.024717355147004128\n",
      "Epoch: 253 | Loss: 0.010149721056222916 | Test Loss: 0.024717355147004128\n",
      "Epoch: 254 | Loss: 0.010115453973412514 | Test Loss: 0.024717355147004128\n",
      "Epoch: 255 | Loss: 0.010081183165311813 | Test Loss: 0.024717355147004128\n",
      "Epoch: 256 | Loss: 0.010046909563243389 | Test Loss: 0.024717355147004128\n",
      "Epoch: 257 | Loss: 0.010012646205723286 | Test Loss: 0.024717355147004128\n",
      "Epoch: 258 | Loss: 0.009978375397622585 | Test Loss: 0.024717355147004128\n",
      "Epoch: 259 | Loss: 0.009944106452167034 | Test Loss: 0.024717355147004128\n",
      "Epoch: 260 | Loss: 0.009909841232001781 | Test Loss: 0.023187441751360893\n",
      "Epoch: 261 | Loss: 0.009875568561255932 | Test Loss: 0.023187441751360893\n",
      "Epoch: 262 | Loss: 0.009841302409768105 | Test Loss: 0.023187441751360893\n",
      "Epoch: 263 | Loss: 0.009807029739022255 | Test Loss: 0.023187441751360893\n",
      "Epoch: 264 | Loss: 0.009772763587534428 | Test Loss: 0.023187441751360893\n",
      "Epoch: 265 | Loss: 0.009738489985466003 | Test Loss: 0.023187441751360893\n",
      "Epoch: 266 | Loss: 0.00970422476530075 | Test Loss: 0.023187441751360893\n",
      "Epoch: 267 | Loss: 0.00966995395720005 | Test Loss: 0.023187441751360893\n",
      "Epoch: 268 | Loss: 0.009635686874389648 | Test Loss: 0.023187441751360893\n",
      "Epoch: 269 | Loss: 0.009601418860256672 | Test Loss: 0.023187441751360893\n",
      "Epoch: 270 | Loss: 0.00956715177744627 | Test Loss: 0.023187441751360893\n",
      "Epoch: 271 | Loss: 0.00953287910670042 | Test Loss: 0.023187441751360893\n",
      "Epoch: 272 | Loss: 0.009498612955212593 | Test Loss: 0.023187441751360893\n",
      "Epoch: 273 | Loss: 0.009464342147111893 | Test Loss: 0.023187441751360893\n",
      "Epoch: 274 | Loss: 0.009430073201656342 | Test Loss: 0.023187441751360893\n",
      "Epoch: 275 | Loss: 0.009395807050168514 | Test Loss: 0.023187441751360893\n",
      "Epoch: 276 | Loss: 0.009361536242067814 | Test Loss: 0.023187441751360893\n",
      "Epoch: 277 | Loss: 0.009327266365289688 | Test Loss: 0.023187441751360893\n",
      "Epoch: 278 | Loss: 0.00929300393909216 | Test Loss: 0.023187441751360893\n",
      "Epoch: 279 | Loss: 0.009258733130991459 | Test Loss: 0.023187441751360893\n",
      "Epoch: 280 | Loss: 0.009224463254213333 | Test Loss: 0.02165757492184639\n",
      "Epoch: 281 | Loss: 0.009190193377435207 | Test Loss: 0.02165757492184639\n",
      "Epoch: 282 | Loss: 0.009155924431979656 | Test Loss: 0.02165757492184639\n",
      "Epoch: 283 | Loss: 0.009121653623878956 | Test Loss: 0.02165757492184639\n",
      "Epoch: 284 | Loss: 0.00908738374710083 | Test Loss: 0.02165757492184639\n",
      "Epoch: 285 | Loss: 0.009053120389580727 | Test Loss: 0.02165757492184639\n",
      "Epoch: 286 | Loss: 0.009018848650157452 | Test Loss: 0.02165757492184639\n",
      "Epoch: 287 | Loss: 0.008984582498669624 | Test Loss: 0.02165757492184639\n",
      "Epoch: 288 | Loss: 0.008950311690568924 | Test Loss: 0.02165757492184639\n",
      "Epoch: 289 | Loss: 0.00891605019569397 | Test Loss: 0.02165757492184639\n",
      "Epoch: 290 | Loss: 0.008881776593625546 | Test Loss: 0.02165757492184639\n",
      "Epoch: 291 | Loss: 0.00884750671684742 | Test Loss: 0.02165757492184639\n",
      "Epoch: 292 | Loss: 0.00881323404610157 | Test Loss: 0.02165757492184639\n",
      "Epoch: 293 | Loss: 0.008778966963291168 | Test Loss: 0.02165757492184639\n",
      "Epoch: 294 | Loss: 0.008744706399738789 | Test Loss: 0.02165757492184639\n",
      "Epoch: 295 | Loss: 0.008710428141057491 | Test Loss: 0.02165757492184639\n",
      "Epoch: 296 | Loss: 0.008676156401634216 | Test Loss: 0.02165757492184639\n",
      "Epoch: 297 | Loss: 0.00864188838750124 | Test Loss: 0.02165757492184639\n",
      "Epoch: 298 | Loss: 0.008607624098658562 | Test Loss: 0.02165757492184639\n",
      "Epoch: 299 | Loss: 0.008573357947170734 | Test Loss: 0.02165757492184639\n",
      "Epoch: 300 | Loss: 0.008539083413779736 | Test Loss: 0.02012770250439644\n",
      "Epoch: 301 | Loss: 0.00850490853190422 | Test Loss: 0.02012770250439644\n",
      "Epoch: 302 | Loss: 0.008470648899674416 | Test Loss: 0.02012770250439644\n",
      "Epoch: 303 | Loss: 0.008436378091573715 | Test Loss: 0.02012770250439644\n",
      "Epoch: 304 | Loss: 0.00840210821479559 | Test Loss: 0.02012770250439644\n",
      "Epoch: 305 | Loss: 0.008367840200662613 | Test Loss: 0.02012770250439644\n",
      "Epoch: 306 | Loss: 0.008333572186529636 | Test Loss: 0.02012770250439644\n",
      "Epoch: 307 | Loss: 0.008299300447106361 | Test Loss: 0.02012770250439644\n",
      "Epoch: 308 | Loss: 0.00826506968587637 | Test Loss: 0.02012770250439644\n",
      "Epoch: 309 | Loss: 0.008230864070355892 | Test Loss: 0.02012770250439644\n",
      "Epoch: 310 | Loss: 0.008196590468287468 | Test Loss: 0.02012770250439644\n",
      "Epoch: 311 | Loss: 0.00816232617944479 | Test Loss: 0.02012770250439644\n",
      "Epoch: 312 | Loss: 0.00812805537134409 | Test Loss: 0.02012770250439644\n",
      "Epoch: 313 | Loss: 0.008093779906630516 | Test Loss: 0.02012770250439644\n",
      "Epoch: 314 | Loss: 0.008059518411755562 | Test Loss: 0.02012770250439644\n",
      "Epoch: 315 | Loss: 0.00802525132894516 | Test Loss: 0.02012770250439644\n",
      "Epoch: 316 | Loss: 0.007991059683263302 | Test Loss: 0.02012770250439644\n",
      "Epoch: 317 | Loss: 0.007956808432936668 | Test Loss: 0.02012770250439644\n",
      "Epoch: 318 | Loss: 0.007922538556158543 | Test Loss: 0.02012770250439644\n",
      "Epoch: 319 | Loss: 0.007888269610702991 | Test Loss: 0.02012770250439644\n",
      "Epoch: 320 | Loss: 0.007854007184505463 | Test Loss: 0.018494738265872\n",
      "Epoch: 321 | Loss: 0.007819737307727337 | Test Loss: 0.018494738265872\n",
      "Epoch: 322 | Loss: 0.0077854641713202 | Test Loss: 0.018494738265872\n",
      "Epoch: 323 | Loss: 0.0077512264251708984 | Test Loss: 0.018494738265872\n",
      "Epoch: 324 | Loss: 0.007717023137956858 | Test Loss: 0.018494738265872\n",
      "Epoch: 325 | Loss: 0.0076827555894851685 | Test Loss: 0.018494738265872\n",
      "Epoch: 326 | Loss: 0.0076484899036586285 | Test Loss: 0.018494738265872\n",
      "Epoch: 327 | Loss: 0.007614220026880503 | Test Loss: 0.018494738265872\n",
      "Epoch: 328 | Loss: 0.0075799510814249516 | Test Loss: 0.018494738265872\n",
      "Epoch: 329 | Loss: 0.0075456793420016766 | Test Loss: 0.018494738265872\n",
      "Epoch: 330 | Loss: 0.007511411793529987 | Test Loss: 0.018494738265872\n",
      "Epoch: 331 | Loss: 0.007477217819541693 | Test Loss: 0.018494738265872\n",
      "Epoch: 332 | Loss: 0.007442973554134369 | Test Loss: 0.018494738265872\n",
      "Epoch: 333 | Loss: 0.007408702280372381 | Test Loss: 0.018494738265872\n",
      "Epoch: 334 | Loss: 0.0073744370602071285 | Test Loss: 0.018494738265872\n",
      "Epoch: 335 | Loss: 0.0073401653207838535 | Test Loss: 0.018494738265872\n",
      "Epoch: 336 | Loss: 0.007305896375328302 | Test Loss: 0.018494738265872\n",
      "Epoch: 337 | Loss: 0.007271626498550177 | Test Loss: 0.018494738265872\n",
      "Epoch: 338 | Loss: 0.007237383630126715 | Test Loss: 0.018494738265872\n",
      "Epoch: 339 | Loss: 0.007203185465186834 | Test Loss: 0.018494738265872\n",
      "Epoch: 340 | Loss: 0.007168918382376432 | Test Loss: 0.016861779615283012\n",
      "Epoch: 341 | Loss: 0.007134652230888605 | Test Loss: 0.016861779615283012\n",
      "Epoch: 342 | Loss: 0.007100383285433054 | Test Loss: 0.016861779615283012\n",
      "Epoch: 343 | Loss: 0.007066112942993641 | Test Loss: 0.016861779615283012\n",
      "Epoch: 344 | Loss: 0.007031840737909079 | Test Loss: 0.016861779615283012\n",
      "Epoch: 345 | Loss: 0.0069975764490664005 | Test Loss: 0.016861779615283012\n",
      "Epoch: 346 | Loss: 0.00696337316185236 | Test Loss: 0.016861779615283012\n",
      "Epoch: 347 | Loss: 0.006929133087396622 | Test Loss: 0.016861779615283012\n",
      "Epoch: 348 | Loss: 0.0068948655389249325 | Test Loss: 0.016861779615283012\n",
      "Epoch: 349 | Loss: 0.006860598921775818 | Test Loss: 0.016861779615283012\n",
      "Epoch: 350 | Loss: 0.006826329510658979 | Test Loss: 0.016861779615283012\n",
      "Epoch: 351 | Loss: 0.006792058702558279 | Test Loss: 0.016861779615283012\n",
      "Epoch: 352 | Loss: 0.006757793482393026 | Test Loss: 0.016861779615283012\n",
      "Epoch: 353 | Loss: 0.006723541300743818 | Test Loss: 0.016861779615283012\n",
      "Epoch: 354 | Loss: 0.00668934965506196 | Test Loss: 0.016861779615283012\n",
      "Epoch: 355 | Loss: 0.0066550797782838345 | Test Loss: 0.016861779615283012\n",
      "Epoch: 356 | Loss: 0.006620815489441156 | Test Loss: 0.016861779615283012\n",
      "Epoch: 357 | Loss: 0.006586542818695307 | Test Loss: 0.016861779615283012\n",
      "Epoch: 358 | Loss: 0.0065522752702236176 | Test Loss: 0.016861779615283012\n",
      "Epoch: 359 | Loss: 0.0065180035308003426 | Test Loss: 0.016861779615283012\n",
      "Epoch: 360 | Loss: 0.006483737379312515 | Test Loss: 0.015263164415955544\n",
      "Epoch: 361 | Loss: 0.006449527572840452 | Test Loss: 0.015263164415955544\n",
      "Epoch: 362 | Loss: 0.006415294948965311 | Test Loss: 0.015263164415955544\n",
      "Epoch: 363 | Loss: 0.0063810283318161964 | Test Loss: 0.015263164415955544\n",
      "Epoch: 364 | Loss: 0.006346759386360645 | Test Loss: 0.015263164415955544\n",
      "Epoch: 365 | Loss: 0.006312494166195393 | Test Loss: 0.015263164415955544\n",
      "Epoch: 366 | Loss: 0.006278220098465681 | Test Loss: 0.015263164415955544\n",
      "Epoch: 367 | Loss: 0.006243952549993992 | Test Loss: 0.015263164415955544\n",
      "Epoch: 368 | Loss: 0.006209677550941706 | Test Loss: 0.015263164415955544\n",
      "Epoch: 369 | Loss: 0.006175514310598373 | Test Loss: 0.015263164415955544\n",
      "Epoch: 370 | Loss: 0.006141245365142822 | Test Loss: 0.015263164415955544\n",
      "Epoch: 371 | Loss: 0.006106975022703409 | Test Loss: 0.015263164415955544\n",
      "Epoch: 372 | Loss: 0.006072705145925283 | Test Loss: 0.015263164415955544\n",
      "Epoch: 373 | Loss: 0.006038440857082605 | Test Loss: 0.015263164415955544\n",
      "Epoch: 374 | Loss: 0.006004168186336756 | Test Loss: 0.015263164415955544\n",
      "Epoch: 375 | Loss: 0.005969901569187641 | Test Loss: 0.015263164415955544\n",
      "Epoch: 376 | Loss: 0.0059356847777962685 | Test Loss: 0.015263164415955544\n",
      "Epoch: 377 | Loss: 0.005901461001485586 | Test Loss: 0.015263164415955544\n",
      "Epoch: 378 | Loss: 0.005867186468094587 | Test Loss: 0.015263164415955544\n",
      "Epoch: 379 | Loss: 0.0058329240418970585 | Test Loss: 0.015263164415955544\n",
      "Epoch: 380 | Loss: 0.005798657424747944 | Test Loss: 0.013630193658173084\n",
      "Epoch: 381 | Loss: 0.005764387082308531 | Test Loss: 0.013630193658173084\n",
      "Epoch: 382 | Loss: 0.005730117205530405 | Test Loss: 0.013630193658173084\n",
      "Epoch: 383 | Loss: 0.005695851054042578 | Test Loss: 0.013630193658173084\n",
      "Epoch: 384 | Loss: 0.005661680828779936 | Test Loss: 0.013630193658173084\n",
      "Epoch: 385 | Loss: 0.005627412348985672 | Test Loss: 0.013630193658173084\n",
      "Epoch: 386 | Loss: 0.005593137349933386 | Test Loss: 0.013630193658173084\n",
      "Epoch: 387 | Loss: 0.005558869335800409 | Test Loss: 0.013630193658173084\n",
      "Epoch: 388 | Loss: 0.005524601321667433 | Test Loss: 0.013630193658173084\n",
      "Epoch: 389 | Loss: 0.005490331444889307 | Test Loss: 0.013630193658173084\n",
      "Epoch: 390 | Loss: 0.005456078797578812 | Test Loss: 0.013630193658173084\n",
      "Epoch: 391 | Loss: 0.005421840585768223 | Test Loss: 0.013630193658173084\n",
      "Epoch: 392 | Loss: 0.005387621466070414 | Test Loss: 0.013630193658173084\n",
      "Epoch: 393 | Loss: 0.005353356245905161 | Test Loss: 0.013630193658173084\n",
      "Epoch: 394 | Loss: 0.00531908730044961 | Test Loss: 0.013630193658173084\n",
      "Epoch: 395 | Loss: 0.005284816026687622 | Test Loss: 0.013630193658173084\n",
      "Epoch: 396 | Loss: 0.005250544752925634 | Test Loss: 0.013630193658173084\n",
      "Epoch: 397 | Loss: 0.00521628325805068 | Test Loss: 0.013630193658173084\n",
      "Epoch: 398 | Loss: 0.005182010121643543 | Test Loss: 0.013630193658173084\n",
      "Epoch: 399 | Loss: 0.005147834774106741 | Test Loss: 0.013630193658173084\n",
      "Epoch: 400 | Loss: 0.005113569553941488 | Test Loss: 0.011997222900390625\n",
      "Epoch: 401 | Loss: 0.005079306196421385 | Test Loss: 0.011997222900390625\n",
      "Epoch: 402 | Loss: 0.005045033525675535 | Test Loss: 0.011997222900390625\n",
      "Epoch: 403 | Loss: 0.005010765977203846 | Test Loss: 0.011997222900390625\n",
      "Epoch: 404 | Loss: 0.0049764919094741344 | Test Loss: 0.011997222900390625\n",
      "Epoch: 405 | Loss: 0.004942228551954031 | Test Loss: 0.011997222900390625\n",
      "Epoch: 406 | Loss: 0.004907994996756315 | Test Loss: 0.011997222900390625\n",
      "Epoch: 407 | Loss: 0.004873776342719793 | Test Loss: 0.011997222900390625\n",
      "Epoch: 408 | Loss: 0.0048395185731351376 | Test Loss: 0.011997222900390625\n",
      "Epoch: 409 | Loss: 0.004805245902389288 | Test Loss: 0.011997222900390625\n",
      "Epoch: 410 | Loss: 0.004770983010530472 | Test Loss: 0.011997222900390625\n",
      "Epoch: 411 | Loss: 0.004736711736768484 | Test Loss: 0.011997222900390625\n",
      "Epoch: 412 | Loss: 0.004702433943748474 | Test Loss: 0.011997222900390625\n",
      "Epoch: 413 | Loss: 0.004668175708502531 | Test Loss: 0.011997222900390625\n",
      "Epoch: 414 | Loss: 0.00463399151340127 | Test Loss: 0.011997222900390625\n",
      "Epoch: 415 | Loss: 0.004599732346832752 | Test Loss: 0.011997222900390625\n",
      "Epoch: 416 | Loss: 0.004565465729683638 | Test Loss: 0.011997222900390625\n",
      "Epoch: 417 | Loss: 0.0045311967842280865 | Test Loss: 0.011997222900390625\n",
      "Epoch: 418 | Loss: 0.0044969217851758 | Test Loss: 0.011997222900390625\n",
      "Epoch: 419 | Loss: 0.004462660290300846 | Test Loss: 0.011997222900390625\n",
      "Epoch: 420 | Loss: 0.0044283876195549965 | Test Loss: 0.01039863284677267\n",
      "Epoch: 421 | Loss: 0.00439415592700243 | Test Loss: 0.01039863284677267\n",
      "Epoch: 422 | Loss: 0.0043599470518529415 | Test Loss: 0.01039863284677267\n",
      "Epoch: 423 | Loss: 0.004325681831687689 | Test Loss: 0.01039863284677267\n",
      "Epoch: 424 | Loss: 0.004291415214538574 | Test Loss: 0.01039863284677267\n",
      "Epoch: 425 | Loss: 0.004257142543792725 | Test Loss: 0.01039863284677267\n",
      "Epoch: 426 | Loss: 0.004222869873046875 | Test Loss: 0.01039863284677267\n",
      "Epoch: 427 | Loss: 0.004188606049865484 | Test Loss: 0.01039863284677267\n",
      "Epoch: 428 | Loss: 0.004154335241764784 | Test Loss: 0.01039863284677267\n",
      "Epoch: 429 | Loss: 0.00412014639005065 | Test Loss: 0.01039863284677267\n",
      "Epoch: 430 | Loss: 0.004085892345756292 | Test Loss: 0.01039863284677267\n",
      "Epoch: 431 | Loss: 0.004051626659929752 | Test Loss: 0.01039863284677267\n",
      "Epoch: 432 | Loss: 0.0040173581801354885 | Test Loss: 0.01039863284677267\n",
      "Epoch: 433 | Loss: 0.003983089234679937 | Test Loss: 0.01039863284677267\n",
      "Epoch: 434 | Loss: 0.003948816563934088 | Test Loss: 0.01039863284677267\n",
      "Epoch: 435 | Loss: 0.00391455227509141 | Test Loss: 0.01039863284677267\n",
      "Epoch: 436 | Loss: 0.0038803094066679478 | Test Loss: 0.01039863284677267\n",
      "Epoch: 437 | Loss: 0.0038461119402199984 | Test Loss: 0.01039863284677267\n",
      "Epoch: 438 | Loss: 0.0038118453230708838 | Test Loss: 0.01039863284677267\n",
      "Epoch: 439 | Loss: 0.003777572652325034 | Test Loss: 0.01039863284677267\n",
      "Epoch: 440 | Loss: 0.003743298351764679 | Test Loss: 0.00876566767692566\n",
      "Epoch: 441 | Loss: 0.0037090368568897247 | Test Loss: 0.00876566767692566\n",
      "Epoch: 442 | Loss: 0.003674769541248679 | Test Loss: 0.00876566767692566\n",
      "Epoch: 443 | Loss: 0.0036404968705028296 | Test Loss: 0.00876566767692566\n",
      "Epoch: 444 | Loss: 0.0036063052248209715 | Test Loss: 0.00876566767692566\n",
      "Epoch: 445 | Loss: 0.0035720602609217167 | Test Loss: 0.00876566767692566\n",
      "Epoch: 446 | Loss: 0.003537788288667798 | Test Loss: 0.00876566767692566\n",
      "Epoch: 447 | Loss: 0.0035035216715186834 | Test Loss: 0.00876566767692566\n",
      "Epoch: 448 | Loss: 0.0034692578483372927 | Test Loss: 0.00876566767692566\n",
      "Epoch: 449 | Loss: 0.003434985876083374 | Test Loss: 0.00876566767692566\n",
      "Epoch: 450 | Loss: 0.003400716930627823 | Test Loss: 0.00876566767692566\n",
      "Epoch: 451 | Loss: 0.0033664703369140625 | Test Loss: 0.00876566767692566\n",
      "Epoch: 452 | Loss: 0.003332278225570917 | Test Loss: 0.00876566767692566\n",
      "Epoch: 453 | Loss: 0.0032980069518089294 | Test Loss: 0.00876566767692566\n",
      "Epoch: 454 | Loss: 0.00326373428106308 | Test Loss: 0.00876566767692566\n",
      "Epoch: 455 | Loss: 0.0032294734846800566 | Test Loss: 0.00876566767692566\n",
      "Epoch: 456 | Loss: 0.0031951942946761847 | Test Loss: 0.00876566767692566\n",
      "Epoch: 457 | Loss: 0.003160930471494794 | Test Loss: 0.00876566767692566\n",
      "Epoch: 458 | Loss: 0.003126660129055381 | Test Loss: 0.00876566767692566\n",
      "Epoch: 459 | Loss: 0.003092458937317133 | Test Loss: 0.00876566767692566\n",
      "Epoch: 460 | Loss: 0.0030582197941839695 | Test Loss: 0.007132702972739935\n",
      "Epoch: 461 | Loss: 0.0030239492189139128 | Test Loss: 0.007132702972739935\n",
      "Epoch: 462 | Loss: 0.002989682601764798 | Test Loss: 0.007132702972739935\n",
      "Epoch: 463 | Loss: 0.002955415053293109 | Test Loss: 0.007132702972739935\n",
      "Epoch: 464 | Loss: 0.0029211484361439943 | Test Loss: 0.007132702972739935\n",
      "Epoch: 465 | Loss: 0.0028868780937045813 | Test Loss: 0.007132702972739935\n",
      "Epoch: 466 | Loss: 0.002852626144886017 | Test Loss: 0.007132702972739935\n",
      "Epoch: 467 | Loss: 0.00281843775883317 | Test Loss: 0.007132702972739935\n",
      "Epoch: 468 | Loss: 0.0027841664850711823 | Test Loss: 0.007132702972739935\n",
      "Epoch: 469 | Loss: 0.0027499019633978605 | Test Loss: 0.007132702972739935\n",
      "Epoch: 470 | Loss: 0.002715636044740677 | Test Loss: 0.007132702972739935\n",
      "Epoch: 471 | Loss: 0.0026813626755028963 | Test Loss: 0.007132702972739935\n",
      "Epoch: 472 | Loss: 0.0026470960583537817 | Test Loss: 0.007132702972739935\n",
      "Epoch: 473 | Loss: 0.002612825483083725 | Test Loss: 0.007132702972739935\n",
      "Epoch: 474 | Loss: 0.0025786147452890873 | Test Loss: 0.007132702972739935\n",
      "Epoch: 475 | Loss: 0.0025443860795348883 | Test Loss: 0.007132702972739935\n",
      "Epoch: 476 | Loss: 0.0025101155042648315 | Test Loss: 0.007132702972739935\n",
      "Epoch: 477 | Loss: 0.0024758458603173494 | Test Loss: 0.007132702972739935\n",
      "Epoch: 478 | Loss: 0.0024415755178779364 | Test Loss: 0.007132702972739935\n",
      "Epoch: 479 | Loss: 0.0024073116946965456 | Test Loss: 0.007132702972739935\n",
      "Epoch: 480 | Loss: 0.002373040420934558 | Test Loss: 0.005534106399863958\n",
      "Epoch: 481 | Loss: 0.0023387812543660402 | Test Loss: 0.005534106399863958\n",
      "Epoch: 482 | Loss: 0.002304601017385721 | Test Loss: 0.005534106399863958\n",
      "Epoch: 483 | Loss: 0.002270332770422101 | Test Loss: 0.005534106399863958\n",
      "Epoch: 484 | Loss: 0.0022360682487487793 | Test Loss: 0.005534106399863958\n",
      "Epoch: 485 | Loss: 0.002201793482527137 | Test Loss: 0.005534106399863958\n",
      "Epoch: 486 | Loss: 0.002167528960853815 | Test Loss: 0.005534106399863958\n",
      "Epoch: 487 | Loss: 0.0021332583855837584 | Test Loss: 0.005534106399863958\n",
      "Epoch: 488 | Loss: 0.0020989866461604834 | Test Loss: 0.005534106399863958\n",
      "Epoch: 489 | Loss: 0.002064789878204465 | Test Loss: 0.005534106399863958\n",
      "Epoch: 490 | Loss: 0.0020305439829826355 | Test Loss: 0.005534106399863958\n",
      "Epoch: 491 | Loss: 0.0019962824881076813 | Test Loss: 0.005534106399863958\n",
      "Epoch: 492 | Loss: 0.001962012145668268 | Test Loss: 0.005534106399863958\n",
      "Epoch: 493 | Loss: 0.0019277423853054643 | Test Loss: 0.005534106399863958\n",
      "Epoch: 494 | Loss: 0.001893474138341844 | Test Loss: 0.005534106399863958\n",
      "Epoch: 495 | Loss: 0.0018592014675959945 | Test Loss: 0.005534106399863958\n",
      "Epoch: 496 | Loss: 0.001824941486120224 | Test Loss: 0.005534106399863958\n",
      "Epoch: 497 | Loss: 0.0017907113069668412 | Test Loss: 0.005534106399863958\n",
      "Epoch: 498 | Loss: 0.0017564438749104738 | Test Loss: 0.005534106399863958\n",
      "Epoch: 499 | Loss: 0.0017221764428541064 | Test Loss: 0.005534106399863958\n",
      "Epoch: 500 | Loss: 0.0016879290342330933 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 501 | Loss: 0.0016537345945835114 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 502 | Loss: 0.0016194686759263277 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 503 | Loss: 0.0015852019423618913 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 504 | Loss: 0.0015509299701079726 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 505 | Loss: 0.001516656600870192 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 506 | Loss: 0.0014823891688138247 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 507 | Loss: 0.0014481209218502045 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 508 | Loss: 0.0014139190316200256 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 509 | Loss: 0.0013796836137771606 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 510 | Loss: 0.0013454139698296785 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 511 | Loss: 0.0013111435109749436 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 512 | Loss: 0.00127687759231776 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 513 | Loss: 0.001242607831954956 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 514 | Loss: 0.0012083366746082902 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 515 | Loss: 0.0011740841437131166 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 516 | Loss: 0.0011398941278457642 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 517 | Loss: 0.0011056273942813277 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 518 | Loss: 0.0010713607771322131 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 519 | Loss: 0.0010370895033702254 | Test Loss: 0.0038839876651763916\n",
      "Epoch: 520 | Loss: 0.0010028191609308124 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 521 | Loss: 0.0009685576078481972 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 522 | Loss: 0.0009342834237031639 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 523 | Loss: 0.0009000822901725769 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 524 | Loss: 0.0008658386650495231 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 525 | Loss: 0.0008315801969729364 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 526 | Loss: 0.0007973082247190177 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 527 | Loss: 0.0007630378240719438 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 528 | Loss: 0.000728765910025686 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 529 | Loss: 0.0006944984197616577 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 530 | Loss: 0.0006602369248867035 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 531 | Loss: 0.0006260499358177185 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 532 | Loss: 0.0005917914095334709 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 533 | Loss: 0.0005575217655859888 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 534 | Loss: 0.0005232520634308457 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 535 | Loss: 0.0004889845731668174 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 536 | Loss: 0.0004547193821053952 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 537 | Loss: 0.0004204496799502522 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 538 | Loss: 0.0003862328885588795 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 539 | Loss: 0.0003521561739034951 | Test Loss: 0.0022853852715343237\n",
      "Epoch: 540 | Loss: 0.00031793638481758535 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 541 | Loss: 0.00028354450478218496 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 542 | Loss: 0.0002497769892215729 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 543 | Loss: 0.00021632760763168335 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 544 | Loss: 0.0001858137547969818 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 545 | Loss: 0.00018408299365546554 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 546 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 547 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 548 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 549 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 550 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 551 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 552 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 553 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 554 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 555 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 556 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 557 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 558 | Loss: 0.0004666373133659363 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 559 | Loss: 0.0006893694517202675 | Test Loss: 0.0006527125951834023\n",
      "Epoch: 560 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 561 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 562 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 563 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 564 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 565 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 566 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 567 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 568 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 569 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 570 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 571 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 572 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 573 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 574 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 575 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 576 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 577 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 578 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 579 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 580 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 581 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 582 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 583 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 584 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 585 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 586 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 587 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 588 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 589 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 590 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 591 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 592 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 593 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 594 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 595 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 596 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 597 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 598 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 599 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 600 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 601 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 602 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 603 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 604 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 605 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 606 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 607 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 608 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 609 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 610 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 611 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 612 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 613 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 614 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 615 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 616 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 617 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 618 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 619 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 620 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 621 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 622 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 623 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 624 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 625 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 626 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 627 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 628 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 629 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 630 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 631 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 632 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 633 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 634 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 635 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 636 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 637 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 638 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 639 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 640 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 641 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 642 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 643 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 644 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 645 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 646 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 647 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 648 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 649 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 650 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 651 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 652 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 653 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 654 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 655 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 656 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 657 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 658 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 659 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 660 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 661 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 662 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 663 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 664 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 665 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 666 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 667 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 668 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 669 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 670 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 671 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 672 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 673 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 674 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 675 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 676 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 677 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 678 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 679 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 680 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 681 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 682 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 683 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 684 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 685 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 686 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 687 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 688 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 689 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 690 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 691 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 692 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 693 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 694 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 695 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 696 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 697 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 698 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 699 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 700 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 701 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 702 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 703 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 704 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 705 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 706 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 707 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 708 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 709 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 710 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 711 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 712 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 713 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 714 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 715 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 716 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 717 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 718 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 719 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 720 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 721 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 722 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 723 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 724 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 725 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 726 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 727 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 728 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 729 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 730 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 731 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 732 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 733 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 734 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 735 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 736 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 737 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 738 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 739 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 740 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 741 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 742 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 743 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 744 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 745 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 746 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 747 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 748 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 749 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 750 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 751 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 752 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 753 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 754 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 755 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 756 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 757 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 758 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 759 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 760 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 761 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 762 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 763 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 764 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 765 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 766 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 767 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 768 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 769 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 770 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 771 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 772 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 773 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 774 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 775 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 776 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 777 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 778 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 779 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 780 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 781 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 782 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 783 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 784 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 785 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 786 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 787 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 788 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 789 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 790 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 791 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 792 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 793 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 794 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 795 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 796 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 797 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 798 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 799 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 800 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 801 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 802 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 803 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 804 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 805 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 806 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 807 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 808 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 809 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 810 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 811 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 812 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 813 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 814 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 815 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 816 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 817 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 818 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 819 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 820 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 821 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 822 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 823 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 824 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 825 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 826 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 827 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 828 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 829 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 830 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 831 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 832 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 833 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 834 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 835 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 836 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 837 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 838 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 839 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 840 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 841 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 842 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 843 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 844 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 845 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 846 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 847 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 848 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 849 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 850 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 851 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 852 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 853 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 854 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 855 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 856 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 857 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 858 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 859 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 860 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 861 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 862 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 863 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 864 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 865 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 866 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 867 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 868 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 869 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 870 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 871 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 872 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 873 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 874 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 875 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 876 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 877 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 878 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 879 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 880 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 881 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 882 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 883 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 884 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 885 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 886 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 887 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 888 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 889 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 890 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 891 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 892 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 893 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 894 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 895 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 896 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 897 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 898 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 899 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 900 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 901 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 902 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 903 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 904 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 905 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 906 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 907 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 908 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 909 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 910 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 911 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 912 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 913 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 914 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 915 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 916 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 917 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 918 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 919 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 920 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 921 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 922 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 923 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 924 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 925 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 926 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 927 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 928 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 929 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 930 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 931 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 932 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 933 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 934 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 935 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 936 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 937 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 938 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 939 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 940 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 941 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 942 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 943 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 944 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 945 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 946 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 947 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 948 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 949 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 950 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 951 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 952 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 953 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 954 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 955 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 956 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 957 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 958 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 959 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 960 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 961 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 962 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 963 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 964 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 965 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 966 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 967 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 968 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 969 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 970 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 971 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 972 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 973 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 974 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 975 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 976 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 977 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 978 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 979 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 980 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 981 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 982 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 983 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 984 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 985 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 986 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 987 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 988 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 989 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 990 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 991 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 992 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 993 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 994 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 995 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 996 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 997 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 998 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 999 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1000 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1001 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1002 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1003 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1004 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1005 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1006 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1007 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1008 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1009 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1010 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1011 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1012 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1013 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1014 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1015 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1016 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1017 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1018 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1019 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1020 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1021 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1022 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1023 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1024 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1025 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1026 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1027 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1028 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1029 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1030 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1031 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1032 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1033 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1034 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1035 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1036 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1037 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1038 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1039 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1040 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1041 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1042 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1043 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1044 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1045 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1046 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1047 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1048 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1049 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1050 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1051 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1052 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1053 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1054 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1055 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1056 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1057 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1058 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1059 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1060 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1061 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1062 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1063 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1064 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1065 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1066 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1067 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1068 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1069 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1070 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1071 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1072 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1073 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1074 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1075 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1076 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1077 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1078 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1079 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1080 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1081 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1082 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1083 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1084 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1085 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1086 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1087 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1088 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1089 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1090 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1091 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1092 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1093 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1094 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1095 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1096 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1097 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1098 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1099 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1100 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1101 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1102 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1103 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1104 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1105 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1106 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1107 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1108 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1109 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1110 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1111 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1112 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1113 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1114 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1115 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1116 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1117 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1118 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1119 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1120 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1121 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1122 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1123 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1124 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1125 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1126 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1127 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1128 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1129 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1130 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1131 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1132 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1133 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1134 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1135 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1136 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1137 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1138 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1139 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1140 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1141 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1142 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1143 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1144 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1145 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1146 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1147 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1148 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1149 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1150 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1151 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1152 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1153 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1154 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1155 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1156 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1157 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1158 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1159 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1160 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1161 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1162 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1163 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1164 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1165 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1166 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1167 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1168 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1169 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1170 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1171 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1172 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1173 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1174 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1175 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1176 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1177 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1178 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1179 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1180 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1181 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1182 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1183 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1184 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1185 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1186 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1187 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1188 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1189 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1190 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1191 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1192 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1193 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1194 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1195 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1196 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1197 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1198 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1199 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1200 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1201 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1202 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1203 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1204 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1205 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1206 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1207 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1208 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1209 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1210 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1211 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1212 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1213 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1214 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1215 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1216 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1217 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1218 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1219 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1220 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1221 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1222 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1223 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1224 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1225 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1226 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1227 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1228 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1229 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1230 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1231 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1232 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1233 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1234 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1235 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1236 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1237 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1238 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1239 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1240 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1241 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1242 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1243 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1244 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1245 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1246 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1247 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1248 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1249 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1250 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1251 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1252 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1253 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1254 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1255 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1256 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1257 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1258 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1259 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1260 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1261 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1262 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1263 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1264 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1265 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1266 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1267 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1268 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1269 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1270 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1271 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1272 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1273 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1274 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1275 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1276 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1277 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1278 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1279 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1280 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1281 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1282 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1283 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1284 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1285 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1286 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1287 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1288 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1289 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1290 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1291 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1292 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1293 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1294 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1295 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1296 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1297 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1298 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1299 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1300 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1301 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1302 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1303 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1304 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1305 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1306 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1307 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1308 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1309 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1310 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1311 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1312 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1313 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1314 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1315 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1316 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1317 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1318 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1319 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1320 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1321 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1322 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1323 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1324 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1325 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1326 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1327 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1328 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1329 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1330 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1331 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1332 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1333 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1334 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1335 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1336 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1337 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1338 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1339 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1340 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1341 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1342 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1343 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1344 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1345 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1346 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1347 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1348 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1349 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1350 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1351 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1352 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1353 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1354 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1355 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1356 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1357 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1358 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1359 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1360 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1361 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1362 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1363 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1364 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1365 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1366 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1367 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1368 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1369 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1370 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1371 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1372 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1373 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1374 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1375 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1376 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1377 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1378 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1379 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1380 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1381 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1382 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1383 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1384 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1385 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1386 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1387 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1388 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1389 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1390 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1391 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1392 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1393 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1394 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1395 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1396 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1397 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1398 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1399 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1400 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1401 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1402 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1403 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1404 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1405 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1406 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1407 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1408 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1409 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1410 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1411 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1412 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1413 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1414 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1415 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1416 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1417 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1418 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1419 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1420 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1421 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1422 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1423 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1424 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1425 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1426 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1427 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1428 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1429 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1430 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1431 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1432 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1433 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1434 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1435 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1436 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1437 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1438 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1439 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1440 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1441 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1442 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1443 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1444 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1445 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1446 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1447 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1448 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1449 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1450 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1451 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1452 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1453 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1454 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1455 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1456 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1457 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1458 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1459 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1460 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1461 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1462 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1463 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1464 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1465 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1466 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1467 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1468 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1469 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1470 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1471 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1472 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1473 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1474 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1475 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1476 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1477 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1478 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1479 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1480 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1481 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1482 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1483 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1484 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1485 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1486 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1487 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1488 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1489 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1490 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1491 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1492 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1493 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1494 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1495 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1496 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1497 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1498 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1499 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1500 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1501 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1502 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1503 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1504 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1505 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1506 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1507 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1508 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1509 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1510 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1511 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1512 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1513 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1514 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1515 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1516 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1517 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1518 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1519 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1520 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1521 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1522 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1523 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1524 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1525 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1526 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1527 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1528 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1529 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1530 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1531 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1532 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1533 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1534 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1535 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1536 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1537 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1538 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1539 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1540 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1541 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1542 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1543 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1544 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1545 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1546 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1547 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1548 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1549 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1550 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1551 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1552 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1553 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1554 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1555 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1556 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1557 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1558 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1559 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1560 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1561 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1562 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1563 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1564 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1565 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1566 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1567 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1568 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1569 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1570 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1571 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1572 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1573 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1574 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1575 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1576 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1577 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1578 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1579 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1580 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1581 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1582 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1583 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1584 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1585 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1586 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1587 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1588 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1589 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1590 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1591 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1592 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1593 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1594 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1595 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1596 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1597 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1598 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1599 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1600 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1601 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1602 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1603 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1604 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1605 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1606 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1607 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1608 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1609 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1610 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1611 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1612 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1613 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1614 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1615 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1616 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1617 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1618 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1619 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1620 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1621 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1622 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1623 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1624 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1625 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1626 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1627 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1628 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1629 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1630 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1631 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1632 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1633 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1634 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1635 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1636 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1637 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1638 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1639 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1640 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1641 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1642 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1643 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1644 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1645 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1646 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1647 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1648 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1649 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1650 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1651 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1652 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1653 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1654 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1655 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1656 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1657 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1658 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1659 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1660 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1661 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1662 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1663 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1664 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1665 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1666 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1667 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1668 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1669 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1670 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1671 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1672 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1673 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1674 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1675 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1676 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1677 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1678 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1679 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1680 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1681 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1682 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1683 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1684 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1685 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1686 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1687 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1688 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1689 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1690 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1691 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1692 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1693 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1694 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1695 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1696 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1697 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1698 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1699 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1700 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1701 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1702 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1703 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1704 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1705 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1706 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1707 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1708 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1709 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1710 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1711 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1712 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1713 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1714 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1715 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1716 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1717 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1718 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1719 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1720 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1721 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1722 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1723 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1724 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1725 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1726 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1727 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1728 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1729 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1730 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1731 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1732 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1733 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1734 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1735 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1736 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1737 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1738 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1739 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1740 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1741 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1742 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1743 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1744 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1745 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1746 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1747 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1748 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1749 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1750 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1751 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1752 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1753 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1754 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1755 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1756 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1757 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1758 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1759 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1760 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1761 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1762 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1763 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1764 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1765 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1766 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1767 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1768 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1769 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1770 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1771 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1772 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1773 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1774 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1775 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1776 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1777 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1778 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1779 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1780 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1781 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1782 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1783 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1784 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1785 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1786 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1787 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1788 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1789 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1790 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1791 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1792 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1793 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1794 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1795 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1796 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1797 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1798 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1799 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1800 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1801 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1802 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1803 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1804 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1805 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1806 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1807 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1808 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1809 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1810 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1811 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1812 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1813 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1814 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1815 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1816 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1817 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1818 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1819 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1820 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1821 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1822 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1823 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1824 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1825 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1826 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1827 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1828 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1829 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1830 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1831 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1832 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1833 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1834 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1835 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1836 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1837 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1838 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1839 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1840 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1841 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1842 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1843 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1844 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1845 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1846 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1847 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1848 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1849 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1850 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1851 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1852 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1853 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1854 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1855 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1856 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1857 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1858 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1859 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1860 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1861 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1862 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1863 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1864 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1865 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1866 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1867 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1868 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1869 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1870 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1871 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1872 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1873 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1874 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1875 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1876 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1877 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1878 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1879 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1880 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1881 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1882 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1883 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1884 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1885 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1886 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1887 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1888 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1889 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1890 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1891 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1892 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1893 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1894 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1895 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1896 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1897 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1898 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1899 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1900 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1901 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1902 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1903 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1904 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1905 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1906 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1907 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1908 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1909 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1910 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1911 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1912 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1913 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1914 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1915 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1916 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1917 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1918 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1919 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1920 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1921 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1922 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1923 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1924 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1925 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1926 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1927 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1928 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1929 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1930 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1931 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1932 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1933 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1934 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1935 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1936 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1937 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1938 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1939 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1940 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1941 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1942 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1943 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1944 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1945 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1946 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1947 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1948 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1949 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1950 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1951 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1952 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1953 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1954 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1955 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1956 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1957 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1958 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1959 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1960 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1961 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1962 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1963 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1964 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1965 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1966 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1967 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1968 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1969 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1970 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1971 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1972 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1973 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1974 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1975 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1976 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1977 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1978 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1979 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1980 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1981 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1982 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1983 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1984 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1985 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1986 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1987 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1988 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1989 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1990 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1991 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1992 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1993 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1994 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1995 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1996 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1997 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1998 | Loss: 0.0004666373133659363 | Test Loss: 0.0004941761726513505\n",
      "Epoch: 1999 | Loss: 0.0006893694517202675 | Test Loss: 0.0004941761726513505\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Model dot train\n",
    "    model_0.train()\n",
    "    # Do the forward Pass\n",
    "    y_preds = model_0(X_train)\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_preds,y_train)\n",
    "    # Optimizer Zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "    # Optimizer Step\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        # Testing\n",
    "        # Model dot eval\n",
    "        model_0.eval()\n",
    "        # With torch inference mode\n",
    "        with torch.inference_mode():\n",
    "            y_test_preds = model_0(X_test)\n",
    "            test_loss = loss_fn(y_test_preds,y_test)\n",
    "    # Print out what's happening\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Make predictions with the trained model on the test data.\n",
    "\n",
    "1. Visualize these predictions against the original training and testing data (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAJGCAYAAACZel7oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTEElEQVR4nO3de3RU5aH+8WcSyAVz45rrQBAioCIIaho1hfTgiUI5gPqTQqvBKh5bvGDKUdJSAW2Lp1UJVeoFK/GKIEaOSy2IqSmgeENyqkegIglgDHfMEJAEZvbvjzFjJsxMZpJM5vb9rJVFZs/eO+/oXpjH/b7PNhmGYQgAAAAAIkxUoAcAAAAAAIFAGAIAAAAQkQhDAAAAACISYQgAAABARCIMAQAAAIhIhCEAAAAAEYkwBAAAACAidQv0ADqLzWbT119/rcTERJlMpkAPBwAAAECAGIahY8eOKSMjQ1FR7u//hE0Y+vrrr2U2mwM9DAAAAABBYu/evcrKynL7ftiEocTEREn2D5yUlBTg0QAAAAAIFIvFIrPZ7MgI7oRNGGqeGpeUlEQYAgAAANDm8hkKFAAAAABEJMIQAAAAgIhEGAIAAAAQkcJmzZA3bDabmpqaAj0MBLnu3bsrOjo60MMAAACAn0VMGGpqalJ1dbVsNlugh4IQkJKSorS0NJ5ZBQAAEMYiIgwZhqG6ujpFR0fLbDZ7fPASIpthGDpx4oQOHDggSUpPTw/wiAAAAOAvERGGTp8+rRMnTigjI0M9evQI9HAQ5OLj4yVJBw4cUL9+/ZgyBwAAEKYi4haJ1WqVJMXExAR4JAgVzaH51KlTAR4JAAAA/CUiwlAz1n/AW1wrAAAA4S+iwhAAAAAANCMMRZjs7GyVlpZ6vX9lZaVMJpO++eYbv43JnbKyMqWkpHT5zwUAAEBkIAwFKZPJ5PFrwYIF7TrvRx99pFtuucXr/S+99FLV1dUpOTm5XT+vq/ka9gAAABC5IqJNrrNYrdLGjVJdnZSeLuXnS/4qGqurq3N8v3LlSt17773asWOHY1tCQoLje8MwZLVa1a1b2/86+/bt69M4YmJilJaW5tMxAAAAQCjw+c7Qhg0bNHHiRGVkZMhkMmnNmjUe9y8vL9cVV1yhvn37KikpSXl5eVq3bt0Z+y1dulTZ2dmKi4tTbm6uPvzwQ1+H5lfl5VJ2tlRQIE2fbv8zO9u+3R/S0tIcX8nJyTKZTI7X27dvV2Jiov72t79p9OjRio2N1aZNm/Tll19q0qRJSk1NVUJCgi6++GK9/fbbTudtfefEZDLpqaee0pQpU9SjRw/l5OTotddec7zfeppc89S1devWadiwYUpISNCVV17pFN5Onz6tO+64QykpKerdu7fuueceFRUVafLkyR4/c1lZmfr3768ePXpoypQpOnz4sNP7bX2+sWPHavfu3brrrrscd9Ak6fDhw5o2bZoyMzPVo0cPDR8+XCtWrPDlXwcAAADCkM9h6Pjx4xoxYoSWLl3q1f4bNmzQFVdcoTfffFNbtmxRQUGBJk6cqK1btzr2WblypYqLizV//nx98sknGjFihAoLCx0Pvgy08nLp2mulr75y3l5ba9/ur0DUlrlz5+qBBx7Qtm3bdMEFF6ihoUHjx49XRUWFtm7dqiuvvFITJ07Unj17PJ5n4cKFuu666/TPf/5T48eP109/+lMdOXLE7f4nTpzQgw8+qOeee04bNmzQnj17NGfOHMf7//3f/60XXnhBy5cv17vvviuLxdJmaP7ggw9000036bbbblNVVZUKCgr0u9/9zmmftj5feXm5srKydN9996murs4R0E6ePKnRo0frjTfe0GeffaZbbrlF119/fdAFbgAAAHQxowMkGa+++qrPx5177rnGwoULHa8vueQSY9asWY7XVqvVyMjIMBYtWuT1Oevr6w1JRn19/Rnvffvtt8bnn39ufPvttz6P9fRpw8jKMgzJ9ZfJZBhms30/f1m+fLmRnJzseP3OO+8Ykow1a9a0eex5551nPPLII47XAwYMMBYvXux4LcmYN2+e43VDQ4Mhyfjb3/7m9LOOHj3qGIskY+fOnY5jli5daqSmpjpep6amGn/6058cr0+fPm3079/fmDRpkttxTps2zRg/frzTtqlTpzp97vZ8PncmTJhg/OpXv3L7fkeuGQAAAASWp2zQUpcXKNhsNh07dky9evWSJDU1NWnLli0aN26cY5+oqCiNGzdOmzdvdnuexsZGWSwWpy9/2LjxzDtCLRmGtHevfb+udtFFFzm9bmho0Jw5czRs2DClpKQoISFB27Zta/PO0AUXXOD4/qyzzlJSUpLHu3I9evTQoEGDHK/T09Md+9fX12v//v265JJLHO9HR0dr9OjRHsewbds25ebmOm3Ly8vrlM9ntVp1//33a/jw4erVq5cSEhK0bt26No8DAABAeOvyAoUHH3xQDQ0Nuu666yRJhw4dktVqVWpqqtN+qamp2r59u9vzLFq0SAsXLvTrWCV7WUJn7teZzjrrLKfXc+bM0fr16/Xggw9q8ODBio+P17XXXqumpiaP5+nevbvTa5PJJJvN5tP+hmH4OHrftffz/elPf9KSJUtUWlqq4cOH66yzztLs2bPbPA4AAABts9qs2rhno+qO1Sk9MV35/fMVHeWnlrFO1qVh6MUXX9TChQv1P//zP+rXr1+HzlVSUqLi4mLHa4vFIrPZ3NEhniE9vXP386d3331XM2bM0JQpUyTZ76TU1NR06RiSk5OVmpqqjz76SD/84Q8l2e/MfPLJJxo5cqTb44YNG6YPPvjAadv777/v9NqbzxcTEyOr1XrGcZMmTdLPfvYzSfa7k//617907rnntucjAgAA4Dvl28p115t3aOCntUpvkOoSpOrhmVo8/s+6etjVgR5em7osDL300ku6+eab9fLLLztNievTp4+io6O1f/9+p/3379/vsdI5NjZWsbGxfhtvs/x8KSvLXpbg6uaHyWR/Pz/f70NpU05OjsrLyzVx4kSZTCb99re/9XiHx19uv/12LVq0SIMHD9bQoUP1yCOP6OjRo452N1fuuOMOXXbZZXrwwQc1adIkrVu3TmvXrnXax5vPl52drQ0bNugnP/mJYmNj1adPH+Xk5Gj16tV677331LNnTz388MPav38/YQgAAKADyreV64UF12jTWsncYsXK3qRazf7wGmnBK0EfiLpkzdCKFSt04403asWKFZowYYLTezExMRo9erQqKioc22w2myoqKs5YMxII0dHSkiX271v/Lt/8urTUf88b8sXDDz+snj176tJLL9XEiRNVWFioUaNGdfk47rnnHk2bNk033HCD8vLylJCQoMLCQsXFxbk95gc/+IGWLVumJUuWaMSIEXrrrbc0b948p328+Xz33XefampqNGjQIMczlebNm6dRo0apsLBQY8eOVVpaWps13wAAAHDParPqb3+8RS+vkjJbLd3PtEgvr5LW/vEWWW1W1ycIEibDx8UeDQ0N2rlzpyTpwgsv1MMPP6yCggL16tVL/fv3V0lJiWpra/Xss89Ksk+NKyoq0pIlS3T11d8nw/j4eCUnJ0uyV2sXFRXpiSee0CWXXKLS0lKtWrVK27dvP2MtkTsWi0XJycmqr69XUlKS03snT55UdXW1Bg4c6PEXck/Ky6U773QuUzCb7UHo6uAOvAFns9k0bNgwXXfddbr//vsDPRyvdMY1AwAAEK4qv6zQoFHjlGlxfXfFJumrJGnXJ29r7KB/6+rhecwGLfk8Te7jjz9WQUGB43Xzup2ioiKVlZWprq7OqaXrySef1OnTpzVr1izNmjXLsb15f0maOnWqDh48qHvvvVf79u3TyJEjtXbtWq+DUFe4+mpp0iR7a1xdnX2NUH5+cNwRCja7d+/WW2+9pTFjxqixsVGPPvqoqqurNX369EAPDQAAAO3QuiThVOXfnabGtRYlqb9F+uIflVIAwpC3fA5DY8eO9dgc1hxwmlVWVnp13ttuu0233Xabr8PpUtHR0tixgR5F8IuKilJZWZnmzJkjwzB0/vnn6+2339awYcMCPTQAAAD4yFVJwtDGBF3hxbHpDX4fXod0ebU2wp/ZbNa7774b6GEAAACgg9yVJBzo4V3KGXL+WP8MrJMQhgAAAACcoWVJQmt9TkiG7F/u1gydTOutHmPG+nWMHdUlbXIAAAAAQsvG6krdW35Y0pmhIUr2IGSSPfi0ZJO9dbnH0ieDfoE9d4YAAAAAtKskQZIaeyYq7ugxx3aTOUum0iUhUblMGAIAAAAiXEdKEnbde7vOHXmFo3LZFEKVy4QhAAAAIIJ1uCThgh+FbOUyYQgAAACIUJFQkuAJBQqQJC1YsEAjR44MyM+eMWOGJk+eHJCfDQAAEMkioSTBE8JQkDKZTB6/FixY0KFzr1mzxmnbnDlzVFFR0bFBd5GamhqZTCZVVVUFeigAAAAhxWqzqrKmUis+XaHKmkpHSYK7UBAlexhq6pnotN1kzpJp9SshUZLgCdPkfNC6YSO/f76io/yThOvq6hzfr1y5Uvfee6927Njh2JaQkNCpPy8hIaHTzwkAAIDgEaklCZ5wZ8hL5dvKlb0kWwXPFGh6+XQVPFOg7CXZKt9W7pefl5aW5vhKTk6WyWRy2vbSSy9p2LBhiouL09ChQ/WXv/zFcWxTU5Nuu+02paenKy4uTgMGDNCiRYskSdnZ2ZKkKVOmyGQyOV63nibXPHXtwQcfVHp6unr37q1Zs2bp1KlTjn3q6uo0YcIExcfHa+DAgXrxxReVnZ2t0tJSt5/LarWquLhYKSkp6t27t+6++24ZhuG0z9q1a3X55Zc79vnxj3+sL7/80vH+wIEDJUkXXnihTCaTxn63YO+jjz7SFVdcoT59+ig5OVljxozRJ5984us/egAAgLDjKEm4r1aVz0grXpEqn5Hue83HkoRp0+x/hkEQkghDXinfVq5rV12rryxfOW2vtdTq2lXX+i0QufPCCy/o3nvv1e9//3tt27ZNf/jDH/Tb3/5WzzzzjCTpz3/+s1577TWtWrVKO3bs0AsvvOAIPR999JEkafny5aqrq3O8duWdd97Rl19+qXfeeUfPPPOMysrKVFZW5nj/hhtu0Ndff63Kykq98sorevLJJ3XgwAGPY3/ooYdUVlamp59+Wps2bdKRI0f06quvOu1z/PhxFRcX6+OPP1ZFRYWioqI0ZcoU2Wz22aoffvihJOntt99WXV2dysvt//yPHTumoqIibdq0Se+//75ycnI0fvx4HTt2TAAAAJGqZUlCZqvnBjWXJLReE9TMJulEWm9Fh3BJgidMk2uD1WbVnWvvlCHjjPcMGTLJpNlrZ2vSkEl+mzLX2vz58/XQQw/p6u/maA4cOFCff/65nnjiCRUVFWnPnj3KycnR5ZdfLpPJpAEDBjiO7du3ryQpJSVFaWlpHn9Oz5499eijjyo6OlpDhw7VhAkTVFFRoZkzZ2r79u16++239dFHH+miiy6SJD311FPKycnxeM7S0lKVlJQ4xv74449r3bp1Tvtcc801Tq+ffvpp9e3bV59//rnOP/98x2fo3bu302f40Y9+5HTck08+qZSUFP3jH//Qj3/8Y4/jAgAACFdtlSTY9H1JQsv3w6UkwRPuDLVh456NZ9wRasmQob2Wvdq4Z2OXjOf48eP68ssvddNNNznW+SQkJOh3v/udYyrZjBkzVFVVpSFDhuiOO+7QW2+91a6fdd555ym6xYWfnp7uuPOzY8cOdevWTaNGjXK8P3jwYPXs2dPt+err61VXV6fc3FzHtm7dujnCVLMvvvhC06ZN09lnn62kpCTHXa09e/Z4HO/+/fs1c+ZM5eTkKDk5WUlJSWpoaGjzOAAAgHBCSYL3uDPUhrpjdW3v5MN+HdXQYJ/XuWzZMqdQIckRXEaNGqXq6mr97W9/09tvv63rrrtO48aN0+rVq336Wd27d3d6bTKZHFPV/GnixIkaMGCAli1bpoyMDNlsNp1//vlqamryeFxRUZEOHz6sJUuWaMCAAYqNjVVeXl6bxwEAAIQLShJ8QxhqQ3pieqfu11GpqanKyMjQrl279NOf/tTtfklJSZo6daqmTp2qa6+9VldeeaWOHDmiXr16qXv37rJarR0ax5AhQ3T69Glt3bpVo0ePliTt3LlTR48edXtMcnKy0tPT9cEHH+iHP/yhJOn06dPasmWL4w7T4cOHtWPHDi1btkz5+fmSpE2bNjmdJyYmRpLO+Azvvvuu/vKXv2j8+PGSpL179+rQoUMd+pwAAAChwlGSsFYyt1gbdKCHjyUJEYQw1Ib8/vnKSspSraXW5bohk0zKSspSfv/8LhvTwoULdccddyg5OVlXXnmlGhsb9fHHH+vo0aMqLi7Www8/rPT0dF144YWKiorSyy+/rLS0NKWkpEiyN8pVVFTosssuU2xsrMepbe4MHTpU48aN0y233KLHHntM3bt3169+9SvFx8fLZDK5Pe7OO+/UAw88oJycHA0dOlQPP/ywvvnmG8f7PXv2VO/evfXkk08qPT1de/bs0dy5c53O0a9fP8XHx2vt2rXKyspSXFyckpOTlZOTo+eee04XXXSRLBaL/uu//kvx8fE+fzYAAIBQ07IkobXmkgRDrqfK2SSdTOutHmFakuAJa4baEB0VrSVXLpFkDz4tNb8uvbK0y8oTJOnmm2/WU089peXLl2v48OEaM2aMysrKHJXTiYmJ+uMf/6iLLrpIF198sWpqavTmm28qKsr+r/uhhx7S+vXrZTabdeGFF7Z7HM8++6xSU1P1wx/+UFOmTNHMmTOVmJiouLg4t8f86le/0vXXX6+ioiLl5eUpMTFRU6ZMcbwfFRWll156SVu2bNH555+vu+66S3/605+cztGtWzf9+c9/1hNPPKGMjAxNmjRJkvTXv/5VR48e1ahRo3T99dfrjjvuUL9+/dr9+QAAAEJFWyUJhr4vSWgpEkoSPDEZrR/yEqIsFouSk5NVX1+vpKQkp/dOnjyp6upqDRw40OMv6p6UbyvXnWvvdCpTMCeZVXplqa4eFr6Lynzx1VdfyWw26+2339a//du/BXo4HdIZ1wwAAIC/WG1WbdyzUXXH6pSemK5TFet1xc1/aPO4kz0TFXf0+8eOGOYsmUqXhF1Jgqds0BLT5Lx09bCrNWnIJKeLLr9/fpfeEQo2f//739XQ0KDhw4errq5Od999t7Kzsx3rgQAAAND5KEnoPIQhH0RHRWts9thADyNonDp1Sr/+9a+1a9cuJSYm6tJLL9ULL7xwRgsdAAAAOgclCZ2LMIR2KywsVGFhYaCHAQAAEBEoSeh8hCEAAAAgCLVeF2Q91eSxJMGm70sSWr4f6SUJnhCGAAAAgCDjqrxr4teJes3i/pjmANTYqiTBFKYlCZ2BMAQAAAAEkfJt5bp21bUy2QyN2S1HSULisWNtHyxKEnxBGAIAAACChNVm1Z1r79Tkzw0tOaMkwbtzUJLgPcIQAAAAECQ27tmoi9//SqspSegShCEAAAAgQFqXJHx9dK+WrLW/R0mC/xGGoBkzZuibb77RmjVrJEljx47VyJEjVVpa2u5zdsY5AAAAwpmrkoT/qEvSdC9KEk71SlbMkXrHdkoS2ocwFMRmzJihZ555RpLUvXt39e/fXzfccIN+/etfq1s3//2rKy8v9/rBqZWVlSooKNDRo0eVkpLSrnMAAABEGnclCQnHPCShFrqVPiKZzZQkdBBhyBdWq7Rxo+OiUxdcdFdeeaWWL1+uxsZGvfnmm5o1a5a6d++ukpISp/2ampoUExPTKT+zV69eQXEOAACAcNQZJQlRZjMlCZ3A1doruFJeLmVnSwUF0vTp9j+zs+3b/Sg2NlZpaWkaMGCAfvGLX2jcuHF67bXXNGPGDE2ePFm///3vlZGRoSFDhkiS9u7dq+uuu04pKSnq1auXJk2apJqaGsf5rFariouLlZKSot69e+vuu++WYRhOP3Ps2LGaPXu243VjY6Puuecemc1mxcbGavDgwfrrX/+qmpoaFRQUSJJ69uwpk8mkGTNmuDzH0aNHdcMNN6hnz57q0aOHrrrqKn3xxReO98vKypSSkqJ169Zp2LBhSkhI0JVXXqm6ujrHPpWVlbrkkkt01llnKSUlRZdddpl2797dSf+kAQAAukbLkoTMVjeCmksSbG6ONUyy3xHKz/fzKCMDYcgb5eXStddKX33lvL221r7dz4Gopfj4eDU1NUmSKioqtGPHDq1fv16vv/66Tp06pcLCQiUmJmrjxo169913HaGi+ZiHHnpIZWVlevrpp7Vp0yYdOXJEr776qsefecMNN2jFihX685//rG3btumJJ55QQkKCzGazXnnlFUnSjh07VFdXpyVLlrg8x4wZM/Txxx/rtdde0+bNm2UYhsaPH69Tp0459jlx4oQefPBBPffcc9qwYYP27NmjOXPmSJJOnz6tyZMna8yYMfrnP/+pzZs365ZbbpHJZOrwP1MAAAB/sdqsqqyp1IpPV6iyplJWm1X7vqn1WJJgyF6SYLT6NccwmWSSSSotZUpcJ2GaXFusVunOO6VWd08k2beZTNLs2dKkSX69KA3DUEVFhdatW6fbb79dBw8e1FlnnaWnnnrKMT3u+eefl81m01NPPeUICcuXL1dKSooqKyv17//+7yotLVVJSYmu/m5x3eOPP65169a5/bn/+te/tGrVKq1fv17jxo2TJJ199tmO95unw/Xr189pzVBLX3zxhV577TW9++67uvTSSyVJL7zwgsxms9asWaP/9//+nyTp1KlTevzxxzVo0CBJ0m233ab77rtPkmSxWFRfX68f//jHjveHDRvm+z9IAACALuKqICErKUu/M41zmhrXWnNAaurZqiQhK8sehChJ6DSEobZs3HjmHaGWDEPau9e+nx/mbb7++utKSEjQqVOnZLPZNH36dC1YsECzZs3S8OHDndYJ/e///q927typxMREp3OcPHlSX375perr61VXV6fc3FzHe926ddNFF110xlS5ZlVVVYqOjtaYMWPa/Rm2bdumbt26Of3c3r17a8iQIdq2bZtjW48ePRxBR5LS09N14MABSfbQNWPGDBUWFuqKK67QuHHjdN111yk9Pb3d4wIAAPAXdwUJmwZ8pbX/V6YiL87RuiShK9arRxrCUFtarFnplP18VFBQoMcee0wxMTHKyMhwapE766yznPZtaGjQ6NGj9cILL5xxnr59+7br58fHx7fruPZo3T5nMpmcQtry5ct1xx13aO3atVq5cqXmzZun9evX6wc/+EGXjREAAKAtngoS9iZJy0Z5dx5KEvyPNUNt8fbOg5/uUJx11lkaPHiw+vfv32ad9qhRo/TFF1+oX79+Gjx4sNNXcnKykpOTlZ6erg8++MBxzOnTp7Vlyxa35xw+fLhsNpv+8Y9/uHy/+c6U1Wp1e45hw4bp9OnTTj/38OHD2rFjh84991yPn6m1Cy+8UCUlJXrvvfd0/vnn68UXX/TpeAAAAH/zVJCQaZEWVEqH4u1rg1yhJKHrEIbakp8vZWXZ1wa5YjIFzcX605/+VH369NGkSZO0ceNGVVdXq7KyUnfccYe++m6q35133qkHHnhAa9as0fbt2/XLX/5S33zzjdtzZmdnq6ioSD//+c+1Zs0axzlXrVolSRowYIBMJpNef/11HTx4UA0NDWecIycnR5MmTdLMmTO1adMm/e///q9+9rOfKTMzU5MmTfLqs1VXV6ukpESbN2/W7t279dZbb+mLL75g3RAAAAi41iUJXx/d67EgQfouCJnspQgtUZLQtQhDbYmOlpob0loHoubXQXKx9ujRQxs2bFD//v119dVXa9iwYbrpppt08uRJJSUlSZJ+9atf6frrr1dRUZHy8vKUmJioKVOmeDzvY489pmuvvVa//OUvNXToUM2cOVPHjx+XJGVmZmrhwoWaO3euUlNTddttt7k8x/LlyzV69Gj9+Mc/Vl5engzD0Jtvvun1g1l79Oih7du365prrtE555yjW265RbNmzdJ//ud/+vBPCAAAoHOVbytX9pJsFTxToOnl01XwTIFWPnGbzBb3v2hHSer7rVQz+0aZMjOd3jNlZUmrV1OS0EVMhruV8yHGYrEoOTlZ9fX1jl/8m508eVLV1dUaOHCg4uLi2vcDysvtrXItyxTMZho9wlSnXDMAACCstSxJyG9RkpB5THrBiyev2J5/XlE/+Ym9iIuShE7lKRu0RIGCt66+2l6fzcUKAAAQ8TyVJBzo4d05ojIz7b9LUpIQMIQhX3CxAgAAQM4lCa31OWFfE2TI9VQ5wySZsoJjzXmkIwwBAAAAbbDarNq4Z6PqjtUpPTG9zZIEmySTvgs+LRal2AsSFDRrziMdYQgAAADwoHxbue5ce6e+sny/dvw/6pI03eL+mOaA1NQzWTFH6h3bTVlZrDkPIhEVhsKkKwJdgGsFAABIziUJY1qUJCQc85CEWuhW+oi9dIs150EpIsJQ9HcXXFNTk+Lj4wM8GoSCEydOSJLX1d8AACD8dEpJgtnMmvMgFhFhqFu3burRo4cOHjyo7t27KyqKxyvBNcMwdOLECR04cEApKSmOIA0AACIPJQnhLyLCkMlkUnp6uqqrq7V79+5ADwchICUlRWlpaYEeBgAA6EKUJESeiAhDkhQTE6OcnBw1NTUFeigIct27d+eOEAAAEYaShMgUMWFIkqKiohQXFxfoYQAAACCIUJIQuSIqDAEAAAAtUZIQ2QhDAAAAiFiUJEQ2whAAAAAiBiUJaIkwBAAAgIhASQJaIwwBAAAg7FGSAFcIQwAAAAhrlCTAHcIQAAAAwholCXCHMAQAAICwQkkCvEUYAgAAQNgo31auu968QwM/rXWsC+oZQ0kCXCMMAQAAICyUbyvXCwuu0aZW64IOx1OSANcIQwAAAAh5VptVf/vjLXrZxbqgnt96dw5KEiKPq3ViAAAAQEjZWF2pe8sPS3K9Lqi5JMEVwyT7HSFKEiIOd4YAAAAQclqXJJyq/LvT1LjWTN/9abT4XqIkIdIRhgAAABBSXJUkDG1M0BVeHHs6JVHdvznmeE1JQmQjDAEAACBkuCtJONCjwavjo1atlrrHUJIASYQhAAAAhAhPJQltPTzVJulkWm/1+NG/EX7gQIECAAAAQoI3JQkm2YNPSzZJJpPUY+mTBCE44c4QAAAAgpKvJQnNAamxZ6LijrZYF2TOkql0CeuCcAbCEAAAAIJOR0oSdt17u84deYVjXZCJdUFwgzAEAACAoNLRkoQhF/yIh6fCK4QhAAAABI1OKUkYM9avY0T4oEABAAAAQYOSBHQln8PQhg0bNHHiRGVkZMhkMmnNmjUe96+rq9P06dN1zjnnKCoqSrNnzz5jn7KyMplMJqevuLg4X4cGAACAEGO1WVVZU6kVn65QZU2loyTB3S+pUbKHoaaeiU7bTeYsmVa/QkkCfOLzNLnjx49rxIgR+vnPf66rvbjYGhsb1bdvX82bN0+LFy92u19SUpJ27NjheG0ymXwdGgAAAEIIJQkINJ/D0FVXXaWrrrrK6/2zs7O1ZMkSSdLTTz/tdj+TyaS0tDSvz9vY2KjGxkbHa4vFQ88iAAAAggolCQgGQbNmqKGhQQMGDJDZbNakSZP0f//3fx73X7RokZKTkx1fZrO5i0YKAACAjmhZkpDZ6v9nN5cktF4T1Mwm6URab0VTkoBOEBRhaMiQIXr66af1P//zP3r++edls9l06aWX6quvvnJ7TElJierr6x1fe/fu7cIRAwAAoL0oSUCwCIpq7by8POXl5TleX3rppRo2bJieeOIJ3X///S6PiY2NVWxsbFcNEQAAAO1gtVm1cc9G1R2rU3piuvL758v6j0qnqXGtNQekxp6Jijt6zLHdZM6SqXQJJQnoNEERhlrr3r27LrzwQu3cuTPQQwEAAEA7uSpIqB6eqYfr89o+WJQkwP+CMgxZrVZ9+umnGj9+fKCHAgAAgHZwV5CwN6lWT45arWu8OAclCfA3n8NQQ0OD0x2b6upqVVVVqVevXurfv79KSkpUW1urZ5991rFPVVWV49iDBw+qqqpKMTExOvfccyVJ9913n37wgx9o8ODB+uabb/SnP/1Ju3fv1s0339zBjwcAAICu1rIgobVMi7SwUjrcw6SeJwyXC9htkk6m9VYPShLgZz6HoY8//lgFBQWO18XFxZKkoqIilZWVqa6uTnv27HE65sILL3R8v2XLFr344osaMGCAampqJElHjx7VzJkztW/fPvXs2VOjR4/We++95whLAAAACB1tFSTYJNkMw1GS0HIfShLQlUyGYRiBHkRnsFgsSk5OVn19vZKSkgI9HAAAgIjRuiThVMV6XXHzH9o87p+/vFbDX3tfphYNwgYlCegE3maDoFwzBAAAgNDgqiRhaGOCrvDi2G5DhspU85K0cSMlCQgIwhAAAADaxV1JwoEeDV4dP+T8sfbgQ0kCAoQwBAAAAJ95Kknoc8L+4FRDZ64ZkihIQPBwdX0CAAAAHrVVkmBIjoKElihIQDDhzhAAAADadEZJQuXfnabGtdYckBp7Jiru6DHHdhMFCQgihCEAAAB41JGShF333q5zR15BQQKCEmEIAAAAbnW4JOGCH1GQgKBFGAIAAIBLlCQg3FGgAAAAAJcoSUC4484QAAAAJElWq9PzT3VqZyUlCQhrhCEAAACovFy6807pq6++3zbzbFGSgLBGGAIAAIhw5eXStddKhuG8/V/VYyX9rs3jKUlAqGLNEAAAQASzWu13hFoHIUnaaIzV3m69z1gT1Mwm6URab0VTkoAQRRgCAACIYBs3Ok+Na8mmaN15+snvvm/9HiUJCH2EIQAAgAhitUqVldKKFfY/a2s97/+qrta1ekXf9spy2m4yZ8m0+hVKEhDSWDMEAAAQIVyVJPTp0/Zxr+pqfbRqksZGb6QkAWGFMAQAABAB3JUkHDrk+TiTScrKkvLHRkvRY/02PiAQmCYHAAAQ5jyVJLRkMrl+XVrKTSCEJ8IQAABAmPNUktBS6ylzWVnS6tUsC0L4YpocAABAmLFa7QHou+U9bZYkNFu8WMrM/P44lgUh3BGGAAAAwkh7SxIkexDi2amIJIQhAACAMNHhkoR8/40NCEasGQIAAAgDlCQAviMMAQAAhAFKEgDfMU0OAAAgBFGSAHQcYQgAACDEUJIAdA7CEAAAQAihJAHoPKwZAgAACBGUJACdizAEAAAQIihJADoX0+QAAACCFCUJgH8RhgAAAIIQJQmA/xGGAAAAggwlCUDXYM0QAABAEKEkAeg6hCEAAIAgQkkC0HWYJgcAABAgrQsS8vPt33uDkgSg4whDAAAAAeCqICErS5o507vjKUkAOs5kGG3NSA0NFotFycnJqq+vV1JSUqCHAwAA4Ja7ggSTyb6td2/pyBHX64aaSxKqq7kTBLjjbTZgzRAAAEAX8lSQYBjOxQiUJAD+xTQ5AAAAP2q9Lshq9VyQYBjS4cPSwoXSsmVnTqMrLaUkAegshCEAAAA/cbUuqFcv747NyZFqas4sWOCOENB5CEMAAAB+4G5d0JEj3h2fnm4PPpQkAP5DGAIAAOhk3j441ZXmgoT8/M4fFwBnFCgAAAB0Mm8fnNoaBQlA1+LOEAAAQAe1LkmorfXuuF69nKfNUZAAdC3CEAAAQAe4Kkno08e7Y1etst8BoiABCAzCEAAAQDu5K0k4dMjzcc3rgsaOJfwAgcSaIQAAgHbwtiSBB6cCwYswBAAA0A7eliS0njKXlSWtXs26ICAYME0OAADAC+0tSVi8WMrMZF0QEIwIQwAAAG3oSElCZiYPTgWCFWEIAADAg46WJPDwVCB4sWYIAADADUoSgPBGGAIAAHCDkgQgvDFNDgAA4DuUJACRhTAEAAAgShKASEQYAgAAEY+SBCAysWYIAABENEoSgMhFGAIAABGNkgQgcjFNDgAARBRKEgA0IwwBAICIQUkCgJYIQwAAICJQkgCgNdYMAQCAsEdJAgBXCEMAACDsUZIAwBWmyQEAgLBDSQIAbxCGAABAWKEkAYC3CEMAACBsUJIAwBesGQIAAGGBkgQAviIMAQCAsEBJAgBfMU0OAACEJEoSAHQUYQgAAIQcShIAdAbCEAAACCmUJADoLD6vGdqwYYMmTpyojIwMmUwmrVmzxuP+dXV1mj59us455xxFRUVp9uzZLvd7+eWXNXToUMXFxWn48OF68803fR0aAAAIc5QkAOhMPoeh48ePa8SIEVq6dKlX+zc2Nqpv376aN2+eRowY4XKf9957T9OmTdNNN92krVu3avLkyZo8ebI+++wzX4cHAADCGCUJADqTyTDa+n8rHg42mfTqq69q8uTJXu0/duxYjRw5UqWlpU7bp06dquPHj+v11193bPvBD36gkSNH6vHHH/fq3BaLRcnJyaqvr1dSUpK3HwEAAAQxVyUJP/tZ28c9/zwlCUAk8zYbBMWaoc2bN6u4uNhpW2FhoccpeI2NjWpsbHS8tlgs/hoeAAAIAEoSAPhbUDxnaN++fUpNTXXalpqaqn379rk9ZtGiRUpOTnZ8mc1mfw8TAAB0keaShNZT4rwpSTCbKUkA4J2gCEPtUVJSovr6esfX3r17Az0kAADQCShJANBVgmKaXFpamvbv3++0bf/+/UpLS3N7TGxsrGJjY/09NAAA0MV8KUk4ePD711lZ9iBESQIAbwVFGMrLy1NFRYVT7fb69euVl5cXuEEBAAC/a12QkJ9v/94bixdTkgCgY3wOQw0NDdq5c6fjdXV1taqqqtSrVy/1799fJSUlqq2t1bPPPuvYp6qqynHswYMHVVVVpZiYGJ177rmSpDvvvFNjxozRQw89pAkTJuill17Sxx9/rCeffLKDHw8AAAQrVwUJWVnSzJneHU9JAoCO8rlau7KyUgUFBWdsLyoqUllZmWbMmKGamhpVVlZ+/0NaT+qVNGDAANXU1Dhev/zyy5o3b55qamqUk5OjP/7xjxo/frzX46JaGwCA0NFckND6txCTyb6td2/pyBHX64ZMJntoqq7mThAA17zNBh16zlAwIQwBABAarFYpO9v9uiCTSerVyx6GJOdA1Pz/V3mAKgBPvM0GIdsmBwAAQlNbBQmGIR0+LC1YYJ8K11JWFkEIQOcJigIFAAAQvlqXJNTWendcTo5UU3NmwQJT4wB0FsIQAADwG1clCX36eHdsero9+FCSAMBfCEMAAMAv3JUkHDrk+bjmgoT8fP+NDQAk1gwBAAA/sFrtd4TaqmlqXTjb/Lq0lOlwAPyPMAQAADpdWyUJzVpPmaMgAUBXYpocAADosPaWJCxebG+MoyABQCAQhgAAQId0pCQhM5OCBACBQxgCAADtRkkCgFDGmiEAANAulCQACHWEIQAA0C6UJAAIdUyTAwAAXqEkAUC4IQwBAIA2UZIAIBwRhgAAgEeUJAAIV6wZAgAAblGSACCcEYYAAIBblCQACGdMkwMAAA6UJACIJIQhAAAgiZIEAJGHMAQAAChJABCRWDMEAECEoyQBQKQiDAEAEOEoSQAQqZgmBwBAhKEkAQDsCEMAAEQQShIA4HuEIQAAIgQlCQDgjDVDAABEAEoSAOBMhCEAACIAJQkAcCamyQEAEIYoSQCAthGGAAAIM5QkAIB3CEMAAIQRShIAwHusGQIAIExQkgAAviEMAQAQJihJAADfME0OAIAQRUkCAHQMYQgAgBBESQIAdBxhCACAEENJAgB0DtYMAQAQQihJAIDOQxgCACCEUJIAAJ2HaXIAAASp1gUJ+fn2771BSQIAtI0wBABAEHJVkJCVJc2c6d3xlCQAQNtMhtHWrOPQYLFYlJycrPr6eiUlJQV6OAAAtJu7ggSTyb6td2/pyBHX64aaSxKqq7kTBCByeZsNWDMEAEAQ8VSQYBjOxQiUJABAxxCGAAAIIm0VJBiGdPiwtGCBfSpcS5QkAIBvWDMEAEAAtS5JqK317ricHKmm5syCBe4IAYD3CEMAAASIq5KE1pXY7qSn24MPJQkA0H6EIQAAAsBdScKhQ56Pay5IyM/339gAIFKwZggAgC7mqSShJQoSAMC/CEMAAHSxtkoSmrWeMkdBAgB0LqbJAQDgZ+0tSVi82N4YR0ECAPgHYQgAAD/qSElCZiYFCQDgT4QhAAD8hJIEAAhurBkCAMAPKEkAgOBHGAIAwA8oSQCA4Mc0OQAAOgElCQAQeghDAAB0ECUJABCaCEMAAHQAJQkAELpYMwQAQDtRkgAAoY07QwAAeKn1uiCr1fuShIMHv3+dlWUPQpQkAEBgEYYAAPCCq3VBvXp5dywlCQAQnAhDAAC0wd26oCNHvDuekgQACE6EIQAAPPB2XZArlCQAQHCjQAEAAA+8fXhqa5QkAEDw484QAAAttPfhqb16OU+boyQBAIIfYQgAgO905OGpq1bZ7wBRkgAAoYMwBACAOv7w1LFjCT8AEGpYMwQAiHg8PBUAIhNhCAAQ8bwtSWg9ZS4rS1q9mnVBABCqmCYHAIg47S1J4OGpABBeCEMAgIjSkZIEHp4KAOGFMAQAiBgdLUng4akAEF5YMwQAiAiUJAAAWiMMAQAiAiUJAIDWmCYHAAhLlCQAANpCGAIAhB1KEgAA3vB5mtyGDRs0ceJEZWRkyGQyac2aNW0eU1lZqVGjRik2NlaDBw9WWVmZ0/sLFiyQyWRy+ho6dKivQwMAwFGS0HpKnDclCWYzJQkAEEl8DkPHjx/XiBEjtHTpUq/2r66u1oQJE1RQUKCqqirNnj1bN998s9atW+e033nnnae6ujrH16ZNm3wdGgAgwlGSAADwhc/T5K666ipdddVVXu//+OOPa+DAgXrooYckScOGDdOmTZu0ePFiFRYWfj+Qbt2Ulpbm9XkbGxvV2NjoeG2xWLw+FgAQnnwpSTh48PvXWVn2IERJAgBEFr+vGdq8ebPGjRvntK2wsFCzZ8922vbFF18oIyNDcXFxysvL06JFi9S/f3+35120aJEWLlzojyEDAEIEJQkAgI7wexjat2+fUlNTnbalpqbKYrHo22+/VXx8vHJzc1VWVqYhQ4aorq5OCxcuVH5+vj777DMlJia6PG9JSYmKi4sdry0Wi8xms18/CwAgeFCSAADoqKBok2s57e6CCy5Qbm6uBgwYoFWrVummm25yeUxsbKxiY2O7aogAgCDSXJLQem2QNyUJWVmUJAAA7Pz+0NW0tDTt37/fadv+/fuVlJSk+Ph4l8ekpKTonHPO0c6dO/09PABAiKEkAQDQWfwehvLy8lRRUeG0bf369crLy3N7TENDg7788kulp6f7e3gAgBDjS0lCS1lZ0urVlCQAAL7n8zS5hoYGpzs21dXVqqqqUq9evdS/f3+VlJSotrZWzz77rCTp1ltv1aOPPqq7775bP//5z/X3v/9dq1at0htvvOE4x5w5czRx4kQNGDBAX3/9tebPn6/o6GhNmzatEz4iACBUtS5IyM+3f+8NShIAAG3xOQx9/PHHKigocLxuLjEoKipSWVmZ6urqtGfPHsf7AwcO1BtvvKG77rpLS5YsUVZWlp566imnWu2vvvpK06ZN0+HDh9W3b19dfvnlev/999W3b9+OfDYAQAhzVZCQlSXNnOnd8ZQkAADaYjKMtmZdhwaLxaLk5GTV19crKSkp0MMBAHSAu4IEk8m+rXdv6cgR1+uGmksSqqu5EwQAkcrbbOD3NUMAAPjCU0GCYTgXI1CSAADoCMIQACCotFWQYBjS4cPSggX2qXAtUZIAAPBFUDxnCAAQuVqXJNTWendcTo5UU3NmwQJ3hAAA3iIMAQACxlVJQutKbHfS0+3Bh5IEAEB7EYYAAAHhriTh0CHPxzUXJOTn+29sAIDIwJohAECX81SS0BIFCQAAfyIMAQC6XFslCc1aT5mjIAEA0JmYJgcA8Lv2liQsXmxvjKMgAQDgD4QhAIBfdaQkITOTggQAgP8QhgAAfkNJAgAgmLFmCADgF5QkAACCHWEIAOAXlCQAAIId0+QAAJ2CkgQAQKghDAEAOoySBABAKCIMAQA6hJIEAECoYs0QAKDdKEkAAIQywhAAoN0oSQAAhDKmyQEAvEZJAgAgnBCGAABeoSQBABBuCEMAgDZRkgAACEesGQIAeERJAgAgXBGGAAAeUZIAAAhXTJMDADihJAEAECkIQwAAB0oSAACRhDAEAJBESQIAIPKwZggAQEkCACAiEYYAAJQkAAAiEtPkACACUZIAAABhCAAiDiUJAADYEYYAIIJQkgAAwPdYMwQAEYKSBAAAnBGGACBCUJIAAIAzpskBQJiiJAEAAM8IQwAQhihJAACgbYQhAAgzlCQAAOAd1gwBQBihJAEAAO8RhgAgjFCSAACA95gmBwAhqnVBQn6+/XtvUJIAAABhCABCkquChKwsaeZM746nJAEAAMlkGG3NLA8NFotFycnJqq+vV1JSUqCHAwB+464gwWSyb+vdWzpyxPW6oeaShOpq7gQBAMKXt9mANUMAEEI8FSQYhnMxAiUJAAB4RhgCgBDSVkGCYUiHD0sLFtinwrVESQIAAM5YMwQAQax1SUJtrXfH5eRINTVnFixwRwgAgO8RhgAgSLkqSWhdie1Oero9+FCSAACAe4QhAAhC7koSDh3yfFxzQUJ+vv/GBgBAuGDNEAAEGU8lCS1RkAAAQMcQhgAgyLRVktCs9ZQ5ChIAAPAN0+QAIMDaW5KweLG9MY6CBAAA2ocwBAAB1JGShMxMChIAAOgIwhAABAglCQAABBZrhgAgAChJAAAg8AhDABAAlCQAABB4TJMDgC5ASQIAAMGHMAQAfkZJAgAAwYkwBAB+REkCAADBizVDAOAnlCQAABDcCEMA4CeUJAAAENyYJgcAnYSSBAAAQgthCAA6ASUJAACEHsIQAHQQJQkAAIQm1gwBQAdQkgAAQOgiDAFAB1CSAABA6GKaHAD4gJIEAADCB2EIALxESQIAAOGFMAQAXqAkAQCA8MOaIQBoAyUJAACEJ+4MAUArrdcFWa3elyQcPPj966wsexCiJAEAgOBEGAKAFlytC+rVy7tjKUkAACC0EIYA4Dvu1gUdOeLd8ZQkAAAQWghDACDv1wW5QkkCAAChiQIFAJD3D09tjZIEAABCl89haMOGDZo4caIyMjJkMpm0Zs2aNo+prKzUqFGjFBsbq8GDB6usrOyMfZYuXars7GzFxcUpNzdXH374oa9DAwCvWa1SZaW0YoX9T28fntp6/VBWlrR6NSUJAACEIp+nyR0/flwjRozQz3/+c13txX/9q6urNWHCBN1666164YUXVFFRoZtvvlnp6ekqLCyUJK1cuVLFxcV6/PHHlZubq9LSUhUWFmrHjh3q16+f758KADzoyMNTV62y3wGiJAEAgNBnMoz2zJD/7mCTSa+++qomT57sdp977rlHb7zxhj777DPHtp/85Cf65ptvtHbtWklSbm6uLr74Yj366KOSJJvNJrPZrNtvv11z5871aiwWi0XJycmqr69XUlJSez8SgDDnriShLc3rgqqrCT8AAAQ7b7OB39cMbd68WePGjXPaVlhYqM2bN0uSmpqatGXLFqd9oqKiNG7cOMc+rjQ2NspisTh9AYAnPDwVAAC05PcwtG/fPqWmpjptS01NlcVi0bfffqtDhw7JarW63Gffvn1uz7to0SIlJyc7vsxms1/GDyB8eFuS0HrKHOuCAAAITyFbrV1SUqLi4mLHa4vFQiAC4MRqtQeg5vU93pYk8PBUAAAig9/DUFpamvbv3++0bf/+/UpKSlJ8fLyio6MVHR3tcp+0tDS3542NjVVsbKxfxgwg9HWkJIGHpwIAEBn8Pk0uLy9PFRUVTtvWr1+vvLw8SVJMTIxGjx7ttI/NZlNFRYVjHwDwRXNJQuspcYcOeT7OZJLMZh6eCgBApPA5DDU0NKiqqkpVVVWS7NXZVVVV2rNnjyT79LUbbrjBsf+tt96qXbt26e6779b27dv1l7/8RatWrdJdd93l2Ke4uFjLli3TM888o23btukXv/iFjh8/rhtvvLGDHw9ApKEkAQAAeMvnaXIff/yxCgoKHK+b1+0UFRWprKxMdXV1jmAkSQMHDtQbb7yhu+66S0uWLFFWVpaeeuopxzOGJGnq1Kk6ePCg7r33Xu3bt08jR47U2rVrzyhVAIC2+FKScPDg96+zsuxBiJIEAAAiR4eeMxRMeM4QEHlaFyTk59sfijp9etvHPv88JQkAAIQrb7NByLbJAYhsrgoSsrKkmTO9O56SBAAAwJ0hACGnuSCh9d9eJpN9W+/e0pEjrtcNmUz20FRdzZ0gAADClbfZwO9tcgDQmTwVJBiGczECJQkAAMATwhCAkNJWQYJhSIcPSwsW2KfCtZSVJa1eTUkCAACwY80QgKDWuiShtta743JypJqaMwsWuCMEAACaEYYABC1XJQl9+nh3bHq6PfhQkgAAANwhDAEISu5KEg4d8nxcc0FCfr7/xgYAAMIDa4YABB1PJQktUZAAAAA6gjAEIOi0VZLQrPWUOQoSAACAL5gmByDg2luSsHixvTGOggQAANAehCEAAdWRkoTMTAoSAABA+xGGAAQMJQkAACCQWDMEICAoSQAAAIFGGAIQEJQkAACAQGOaHIAuQUkCAAAINoQhAH5HSQIAAAhGhCEAfkVJAgAACFasGQLgN5QkAACAYEYYAuA3lCQAAIBgxjQ5AJ2GkgQAABBKCEMAOgUlCQAAINQQhgB0GCUJAAAgFLFmCECHUJIAAABCFWEIQIdQkgAAAEIV0+QA+ISSBAAAEC4IQwC8RkkCAAAIJ4QhAF6hJAEAAIQb1gwBaBMlCQAAIBwRhgC0iZIEAAAQjpgmB+AMlCQAAIBIQBgC4ISSBAAAECkIQwAcKEkAAACRhDVDACRRkgAAACIPYQiAJEoSAABA5GGaHBChKEkAAACRjjAERCBKEgAAAAhDQMShJAEAAMCONUNABKEkAQAA4HuEISCCUJIAAADwPabJAWGMkgQAAAD3CENAmKIkAQAAwDPCEBCGKEkAAABoG2uGgDBDSQIAAIB3CENAmKEkAQAAwDtMkwNCWOuChPx8+/feoCQBAABEOsIQEKJcFSRkZUkzZ3p3PCUJAAAg0pkMo62VBaHBYrEoOTlZ9fX1SkpKCvRwAL9yV5BgMtm39e4tHTniet1Qc0lCdTV3ggAAQHjyNhuwZggIMZ4KEgzDuRiBkgQAAAD3CENAiGmrIMEwpMOHpQUL7FPhWqIkAQAA4HusGQKCXOuShNpa747LyZFqas4sWOCOEAAAgB1hCAhirkoSWldiu5Oebg8+lCQAAAC4RhgCgpS7koRDhzwf11yQkJ/vv7EBAACEA9YMAUHIU0lCSxQkAAAAtB9hCAhCbZUkNGs9ZY6CBAAAAO8xTQ4IAu0tSVi82N4YR0ECAACA7whDQIB1pCQhM5OCBAAAgPYiDAEBREkCAABA4LBmCAgQShIAAAACizAEBAglCQAAAIHFNDmgi1CSAAAAEFwIQ0AXoCQBAAAg+BCGAD+jJAEAACA4sWYI8CNKEgAAAIIXYQjwI0oSAAAAghfT5IBOREkCAABA6CAMAZ2EkgQAAIDQQhgCOgElCQAAAKGHNUNAB1GSAAAAEJoIQ0AHUZIAAAAQmpgmB/iIkgQAAIDwQBgCfEBJAgAAQPggDAFeoiQBAAAgvLRrzdDSpUuVnZ2tuLg45ebm6sMPP3S776lTp3Tfffdp0KBBiouL04gRI7R27VqnfRYsWCCTyeT0NXTo0PYMDfALShIAAADCj89haOXKlSouLtb8+fP1ySefaMSIESosLNSBAwdc7j9v3jw98cQTeuSRR/T555/r1ltv1ZQpU7R161an/c477zzV1dU5vjZt2tS+TwT4ASUJAAAA4cdkGG39v25nubm5uvjii/Xoo49Kkmw2m8xms26//XbNnTv3jP0zMjL0m9/8RrNmzXJsu+aaaxQfH6/nn39ekv3O0Jo1a1RVVeX1OBobG9XY2Oh4bbFYZDabVV9fr6SkJF8+EnAGVyUJP/tZ28c9/zwlCQAAAIFmsViUnJzcZjbwac1QU1OTtmzZopKSEse2qKgojRs3Tps3b3Z5TGNjo+Li4py2xcfHn3Hn54svvlBGRobi4uKUl5enRYsWqX///m7HsmjRIi1cuNCX4QNeoSQBAAAgMvg0Te7QoUOyWq1KTU112p6amqp9+/a5PKawsFAPP/ywvvjiC9lsNq1fv17l5eWqq6tz7JObm6uysjKtXbtWjz32mKqrq5Wfn69jx465HUtJSYnq6+sdX3v37vXlowAuNZcktJ4S501JgtlMSQIAAEAo8Xub3JIlSzRz5kwNHTpUJpNJgwYN0o033qinn37asc9VV13l+P6CCy5Qbm6uBgwYoFWrVummm25yed7Y2FjFxsb6e/iIIL6UJLTch5IEAACA0OTTnaE+ffooOjpa+/fvd9q+f/9+paWluTymb9++WrNmjY4fP67du3dr+/btSkhI0Nlnn+3256SkpOicc87Rzp07fRke0CGUJAAAAEQWn8JQTEyMRo8erYqKCsc2m82miooK5eXleTw2Li5OmZmZOn36tF555RVNmjTJ7b4NDQ368ssvlZ6e7svwAJ9YrVJlpbRihf3P2lrvjlu8WHrnHenFF+1/VlcThAAAAEKRz9PkiouLVVRUpIsuukiXXHKJSktLdfz4cd14442SpBtuuEGZmZlatGiRJOmDDz5QbW2tRo4cqdraWi1YsEA2m013332345xz5szRxIkTNWDAAH399deaP3++oqOjNW3atE76mIAzShIAAADgcxiaOnWqDh48qHvvvVf79u3TyJEjtXbtWkepwp49exQV9f0Np5MnT2revHnatWuXEhISNH78eD333HNKSUlx7PPVV19p2rRpOnz4sPr27avLL79c77//vvr27dvxTwi00lyS0HptkDclCVlZlCQAAACEC5+fMxSsvO0SR2SzWqXs7LbXBrkrSWBtEAAAQPDzNhv4tGYICCWt1wQ1P0iVkgQAAABIXVCtDQSCqzVBWVn26XHeWLzYvjaork5KT7dPjaM2GwAAILwQhhB23K0Jqq21PwvIG5QkAAAAhD+mySGseHpwavO26Ojv1wC1ZjJJZjMlCQAAAJGAMISw4s2aIKvVHoxaB6Lm16WlTIkDAACIBIQhhLT2Pjh19mz7VLiWKEkAAACILKwZQsjqyINTJ02SHnzQfieJkgQAAIDIRBhCSOqMB6dGR1OSAAAAEMmYJoeQ46kkoSXWBAEAAMATwhBCDg9OBQAAQGdgmhyCntXqvLbH25IEHpwKAAAATwhDCGodKUngwakAAADwhDCEoNUZJQkAAACAO6wZQlCiJAEAAAD+RhhCUKIkAQAAAP7GNDkEBUoSAAAA0NUIQwg4ShIAAAAQCIQhBBQlCQAAAAgU1gwhYChJAAAAQCARhhAwlCQAAAAgkJgmhy5DSQIAAACCCWEIXYKSBAAAAAQbwhD8jpIEAAAABCPWDMGvKEkAAABAsCIMwa8oSQAAAECwYpocOhUlCQAAAAgVhCF0GkoSAAAAEEoIQ+gUlCQAAAAg1LBmCB1GSQIAAABCEWEIHUZJAgAAAEIR0+TgM0oSAAAAEA4IQ/AJJQkAAAAIF4QheI2SBAAAAIQT1gzBK5QkAAAAINwQhuAVShIAAAAQbpgmB5coSQAAAEC4IwzhDJQkAAAAIBIQhuCEkgQAAABECtYMwYGSBAAAAEQSwhAcKEkAAABAJGGaXASjJAEAAACRjDAUoShJAAAAQKQjDEUgShIAAAAA1gxFHEoSAAAAADvCUIShJAEAAACwY5pcGGtdkJCfb//eG5QkAAAAINwRhsKUq4KErCxp5kzvjqckAQAAAOHOZBhtrR4JDRaLRcnJyaqvr1dSUlKghxNQ7goSTCb7tt69pSNHXK8bai5JqK7mThAAAABCk7fZgDVDYcZTQYJhOBcjUJIAAACASEYYCjNtFSQYhnT4sLRggX0qXEuUJAAAACCSsGYoxLUuSait9e64nByppubMggXuCAEAACBSEIZCmKuShNaV2O6kp9uDDyUJAAAAiFSEoRDlriTh0CHPxzUXJOTn+29sAAAAQChgzVAI8lSS0BIFCQAAAIB7hKEQ1FZJQrPWU+YoSAAAAAC+xzS5ENDekoTFi+2NcRQkAAAAAGciDAW5jpQkZGZSkAAAAAC4QxgKYpQkAAAAAP7DmqEgRUkCAAAA4F+EoSBFSQIAAADgX0yTCxKUJAAAAABdizAUBChJAAAAALoeYSjAKEkAAAAAAoM1QwFESQIAAAAQOIShAKIkAQAAAAgcpsl1IUoSAAAAgOBBGOoilCQAAAAAwYUw1AUoSQAAAACCD2uG/IySBAAAACA4EYb8jJIEAAAAIDgxTa6TUZIAAAAAhIZ23RlaunSpsrOzFRcXp9zcXH344Ydu9z116pTuu+8+DRo0SHFxcRoxYoTWrl3boXMGq/JyKTtbKiiQpk+3/zl7tnfHNpckTJtm/5MgBAAAAPiXz2Fo5cqVKi4u1vz58/XJJ59oxIgRKiws1IEDB1zuP2/ePD3xxBN65JFH9Pnnn+vWW2/VlClTtHXr1nafMxg1lyS0nhLnTUmC2UxJAgAAANDVTIbR1tJ+Z7m5ubr44ov16KOPSpJsNpvMZrNuv/12zZ0794z9MzIy9Jvf/EazZs1ybLvmmmsUHx+v559/vl3ndMVisSg5OVn19fVKSkry5SN1mNVqvyPU1togk8m5SKG5JIG1QQAAAEDn8TYb+HRnqKmpSVu2bNG4ceO+P0FUlMaNG6fNmze7PKaxsVFxcXFO2+Lj47Vp06Z2n7P5vBaLxekrUChJAAAAAEKPTwUKhw4dktVqVWpqqtP21NRUbd++3eUxhYWFevjhh/XDH/5QgwYNUkVFhcrLy2W1Wtt9TklatGiRFi5c6Mvw/aauzrv9KEkAAAAAgoffq7WXLFminJwcDR06VDExMbrtttt04403KiqqYz+6pKRE9fX1jq+9e/d20oh9l57u3X6UJAAAAADBw6dE0qdPH0VHR2v//v1O2/fv36+0tDSXx/Tt21dr1qzR8ePHtXv3bm3fvl0JCQk6++yz231OSYqNjVVSUpLTV6Dk59unvLV+cGozShIAAACA4ONTGIqJidHo0aNVUVHh2Gaz2VRRUaG8vDyPx8bFxSkzM1OnT5/WK6+8okmTJnX4nMEiOlpassT+fetA1Py6tJQ7QQAAAEAw8XmuWnFxsZYtW6ZnnnlG27Zt0y9+8QsdP35cN954oyTphhtuUElJiWP/Dz74QOXl5dq1a5c2btyoK6+8UjabTXfffbfX5wwFV19tL0PIzHTeTkkCAAAAEJx8KlCQpKlTp+rgwYO69957tW/fPo0cOVJr1651FCDs2bPHaT3QyZMnNW/ePO3atUsJCQkaP368nnvuOaWkpHh9zlBx9dXSpEn2djlKEgAAAIDg5vNzhoJVIJ8zBAAAACB4+OU5QwAAAAAQLghDAAAAACISYQgAAABARCIMAQAAAIhIhCEAAAAAEYkwBAAAACAiEYYAAAAARCTCEAAAAICIRBgCAAAAEJEIQwAAAAAiEmEIAAAAQEQiDAEAAACISIQhAAAAABGJMAQAAAAgIhGGAAAAAEQkwhAAAACAiNQt0APoLIZhSJIsFkuARwIAAAAgkJozQXNGcCdswtCxY8ckSWazOcAjAQAAABAMjh07puTkZLfvm4y24lKIsNls+vrrr5WYmCiTyRTQsVgsFpnNZu3du1dJSUkBHQtCB9cN2oPrBu3FtYP24LpBewTiujEMQ8eOHVNGRoaiotyvDAqbO0NRUVHKysoK9DCcJCUl8RcFfMZ1g/bgukF7ce2gPbhu0B5dfd14uiPUjAIFAAAAABGJMAQAAAAgIhGG/CA2Nlbz589XbGxsoIeCEMJ1g/bgukF7ce2gPbhu0B7BfN2ETYECAAAAAPiCO0MAAAAAIhJhCAAAAEBEIgwBAAAAiEiEIQAAAAARiTAEAAAAICIRhtpp6dKlys7OVlxcnHJzc/Xhhx963P/ll1/W0KFDFRcXp+HDh+vNN9/sopEimPhy3Sxbtkz5+fnq2bOnevbsqXHjxrV5nSE8+fr3TbOXXnpJJpNJkydP9u8AEbR8vXa++eYbzZo1S+np6YqNjdU555zDf68ikK/XTWlpqYYMGaL4+HiZzWbdddddOnnyZBeNFsFgw4YNmjhxojIyMmQymbRmzZo2j6msrNSoUaMUGxurwYMHq6yszO/jdIUw1A4rV65UcXGx5s+fr08++UQjRoxQYWGhDhw44HL/9957T9OmTdNNN92krVu3avLkyZo8ebI+++yzLh45AsnX66ayslLTpk3TO++8o82bN8tsNuvf//3fVVtb28UjRyD5et00q6mp0Zw5c5Sfn99FI0Ww8fXaaWpq0hVXXKGamhqtXr1aO3bs0LJly5SZmdnFI0cg+XrdvPjii5o7d67mz5+vbdu26a9//atWrlypX//61108cgTS8ePHNWLECC1dutSr/aurqzVhwgQVFBSoqqpKs2fP1s0336x169b5eaQuGPDZJZdcYsyaNcvx2mq1GhkZGcaiRYtc7n/dddcZEyZMcNqWm5tr/Od//qdfx4ng4ut109rp06eNxMRE45lnnvHXEBGE2nPdnD592rj00kuNp556yigqKjImTZrUBSNFsPH12nnssceMs88+22hqauqqISII+XrdzJo1y/jRj37ktK24uNi47LLL/DpOBC9Jxquvvupxn7vvvts477zznLZNnTrVKCws9OPIXOPOkI+ampq0ZcsWjRs3zrEtKipK48aN0+bNm10es3nzZqf9JamwsNDt/gg/7bluWjtx4oROnTqlXr16+WuYCDLtvW7uu+8+9evXTzfddFNXDBNBqD3Xzmuvvaa8vDzNmjVLqampOv/88/WHP/xBVqu1q4aNAGvPdXPppZdqy5Ytjql0u3bt0ptvvqnx48d3yZgRmoLpd+NuXf4TQ9yhQ4dktVqVmprqtD01NVXbt293ecy+fftc7r9v3z6/jRPBpT3XTWv33HOPMjIyzvjLA+GrPdfNpk2b9Ne//lVVVVVdMEIEq/ZcO7t27dLf//53/fSnP9Wbb76pnTt36pe//KVOnTql+fPnd8WwEWDtuW6mT5+uQ4cO6fLLL5dhGDp9+rRuvfVWpsnBI3e/G1ssFn377beKj4/vsrFwZwgIAQ888IBeeuklvfrqq4qLiwv0cBCkjh07puuvv17Lli1Tnz59Aj0chBibzaZ+/frpySef1OjRozV16lT95je/0eOPPx7ooSGIVVZW6g9/+IP+8pe/6JNPPlF5ebneeOMN3X///YEeGuAV7gz5qE+fPoqOjtb+/fudtu/fv19paWkuj0lLS/Npf4Sf9lw3zR588EE98MADevvtt3XBBRf4c5gIMr5eN19++aVqamo0ceJExzabzSZJ6tatm3bs2KFBgwb5d9AICu35Oyc9PV3du3dXdHS0Y9uwYcO0b98+NTU1KSYmxq9jRuC157r57W9/q+uvv14333yzJGn48OE6fvy4brnlFv3mN79RVBT/3x1ncve7cVJSUpfeFZK4M+SzmJgYjR49WhUVFY5tNptNFRUVysvLc3lMXl6e0/6StH79erf7I/y057qRpD/+8Y+6//77tXbtWl100UVdMVQEEV+vm6FDh+rTTz9VVVWV4+s//uM/HG09ZrO5K4ePAGrP3zmXXXaZdu7c6QjQkvSvf/1L6enpBKEI0Z7r5sSJE2cEnuZAbRiG/waLkBZUvxt3eWVDGHjppZeM2NhYo6yszPj888+NW265xUhJSTH27dtnGIZhXH/99cbcuXMd+7/77rtGt27djAcffNDYtm2bMX/+fKN79+7Gp59+GqiPgADw9bp54IEHjJiYGGP16tVGXV2d4+vYsWOB+ggIAF+vm9Zok4tcvl47e/bsMRITE43bbrvN2LFjh/H6668b/fr1M373u98F6iMgAHy9bubPn28kJiYaK1asMHbt2mW89dZbxqBBg4zrrrsuUB8BAXDs2DFj69atxtatWw1JxsMPP2xs3brV2L17t2EYhjF37lzj+uuvd+y/a9cuo0ePHsZ//dd/Gdu2bTOWLl1qREdHG2vXru3ysROG2umRRx4x+vfvb8TExBiXXHKJ8f777zveGzNmjFFUVOS0/6pVq4xzzjnHiImJMc477zzjjTfe6OIRIxj4ct0MGDDAkHTG1/z587t+4AgoX/++aYkwFNl8vXbee+89Izc314iNjTXOPvts4/e//71x+vTpLh41As2X6+bUqVPGggULjEGDBhlxcXGG2Ww2fvnLXxpHjx7t+oEjYN555x2Xv7M0XytFRUXGmDFjzjhm5MiRRkxMjHH22Wcby5cv7/JxG4ZhmAyDe5gAAAAAIg9rhgAAAABEJMIQAAAAgIhEGAIAAAAQkQhDAAAAACISYQgAAABARCIMAQAAAIhIhCEAAAAAEYkwBAAAACAiEYYAAAAARCTCEAAAAICIRBgCAAAAEJH+P74De26K/pE6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "plot_predictions(predictions=y_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Save your trained model's state_dict() to file.\n",
    "\n",
    "1. Create a new instance of your model class you made in 2. and load in the state_dict() you just saved to it.\n",
    "2. Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"01_Pytorch_Workflow_Excerise_Model.pt\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/01_Pytorch_Workflow_Excerise_Model.pt\n",
      "Saved model parameters: \n",
      "OrderedDict({'linear_layer.weight': tensor([[0.2992]], device='cuda:0'), 'linear_layer.bias': tensor([0.8998], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),f=MODEL_SAVE_PATH)\n",
    "print(f\"Saved model parameters: \\n{model_0.state_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16754/4115364988.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.2992]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.8998], device='cuda:0'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = LinearRegressionModel()\n",
    "model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = model_1(X_test)\n",
    "y_preds == loaded_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
